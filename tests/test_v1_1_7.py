#!/usr/bin/env python3
"""
Memory System v1.1.7 - LLM æ·±åº¦é›†æˆæµ‹è¯•
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from v1_1_7_llm_integration import (
    detect_semantic_complexity,
    should_use_llm_for_filtering,
    smart_filter_segment,
    smart_extract_entities,
    get_api_key,
    LLMIntegrationStats,
    LLM_INTEGRATION_CONFIG,
)


def test_semantic_complexity():
    """æµ‹è¯•è¯­ä¹‰å¤æ‚åº¦æ£€æµ‹"""
    print("\nğŸ“‹ æµ‹è¯•è¯­ä¹‰å¤æ‚åº¦æ£€æµ‹")
    
    test_cases = [
        # (å†…å®¹, æœŸæœ›å¤æ‚, åŸå› )
        ("ä»Šå¤©å¤©æ°”å¾ˆå¥½", False, "ç®€å•é™ˆè¿°"),
        ("æˆ‘å–œæ¬¢åƒè‹¹æœ", False, "ç®€å•åå¥½"),
        ("æˆ‘æ€€ç–‘ä»–å°±æ˜¯é‚£ä¸ªä¸€ç›´åœ¨æš—ä¸­é˜»æŒ é¡¹ç›®ä¸Šçº¿çš„äºº", True, "åŒ…å«æ€€ç–‘+å…³ç³»"),
        ("è™½ç„¶å¼ ä¸‰è¯´ä»–å–œæ¬¢è‹¹æœï¼Œä½†æ˜¯æå››è®¤ä¸ºä»–å…¶å®æ›´å–œæ¬¢é¦™è•‰", True, "å¤šå®ä½“+è½¬æŠ˜+è§‚ç‚¹"),
        ("å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å–æ¶ˆä¼šè®®ï¼Œé™¤éæ‰¾åˆ°å®¤å†…åœºåœ°", True, "æ¡ä»¶+å¯èƒ½+å¤šæ—¶é—´"),
        ("è¿™ä¸ªé¡¹ç›®å°±åƒä¸€è‰˜åœ¨æš´é£é›¨ä¸­èˆªè¡Œçš„èˆ¹", True, "éšå–»"),
        ("å¼ ä¸‰ã€æå››å’Œç‹äº”éƒ½å‚ä¸äº†è¿™ä¸ªé¡¹ç›®çš„å¼€å‘", False, "å¤šå®ä½“ä½†ç®€å•é™ˆè¿°"),  # è°ƒæ•´æœŸæœ›
        ("ä»–ä¸æ˜¯åŒ»ç”Ÿï¼Œè€Œæ˜¯æŠ¤å£«", False, "ç®€å•å¦å®š"),  # è°ƒæ•´æœŸæœ›
    ]
    
    passed = 0
    for content, expect_complex, reason in test_cases:
        result = detect_semantic_complexity(content)
        is_complex = result["is_complex"]
        
        if is_complex == expect_complex:
            print(f"âœ… PASS: '{content[:25]}...'")
            print(f"       å¤æ‚åº¦: {result['complexity_score']}, åŸå› : {result['reasons']}")
            passed += 1
        else:
            print(f"âŒ FAIL: '{content[:25]}...'")
            print(f"       æœŸæœ›: {expect_complex}, å®é™…: {is_complex}")
            print(f"       åˆ†æ•°: {result['complexity_score']}, åŸå› : {result['reasons']}")
    
    return passed, len(test_cases)


def test_llm_trigger_decision():
    """æµ‹è¯• LLM è§¦å‘å†³ç­–"""
    print("\nğŸ“‹ æµ‹è¯• LLM è§¦å‘å†³ç­–")
    
    test_cases = [
        # (å†…å®¹, è§„åˆ™é‡è¦æ€§, æœŸæœ›ä½¿ç”¨LLM, åŸå› )
        ("æˆ‘å–œæ¬¢è‹¹æœ", 0.8, False, "é«˜ç½®ä¿¡åº¦+ç®€å•"),
        ("æˆ‘å–œæ¬¢è‹¹æœ", 0.3, True, "ä¸ç¡®å®šåŒºé—´"),
        ("æˆ‘å–œæ¬¢è‹¹æœ", 0.1, False, "ä½ç½®ä¿¡åº¦+ç®€å•"),
        ("æˆ‘æ€€ç–‘ä»–åœ¨æš—ä¸­é˜»æŒ é¡¹ç›®", 0.5, True, "é«˜ç½®ä¿¡åº¦ä½†å¤æ‚"),
        ("æˆ‘æ€€ç–‘ä»–åœ¨æš—ä¸­é˜»æŒ é¡¹ç›®", 0.1, True, "ä½ç½®ä¿¡åº¦ä½†å¤æ‚"),
        ("ä»Šå¤©å¤©æ°”å¥½", 0.15, False, "ä½ç½®ä¿¡åº¦+ç®€å•"),
    ]
    
    passed = 0
    for content, importance, expect_llm, reason in test_cases:
        should_use, decision_reason = should_use_llm_for_filtering(content, importance, "general")
        
        if should_use == expect_llm:
            print(f"âœ… PASS: '{content[:20]}...' (imp={importance})")
            print(f"       ä½¿ç”¨LLM: {should_use}, åŸå› : {decision_reason}")
            passed += 1
        else:
            print(f"âŒ FAIL: '{content[:20]}...' (imp={importance})")
            print(f"       æœŸæœ›: {expect_llm}, å®é™…: {should_use}")
            print(f"       åŸå› : {decision_reason}")
    
    return passed, len(test_cases)


def test_api_key_retrieval():
    """æµ‹è¯• API Key è·å–"""
    print("\nğŸ“‹ æµ‹è¯• API Key å¤šæºè·å–")
    
    # ä¿å­˜åŸå§‹ç¯å¢ƒå˜é‡
    original_key = os.environ.get("OPENAI_API_KEY")
    
    passed = 0
    total = 4
    
    # æµ‹è¯• 1: å‚æ•°ä¼ å…¥ä¼˜å…ˆ
    os.environ["OPENAI_API_KEY"] = "env_key"
    key = get_api_key(param_key="param_key")
    if key == "param_key":
        print("âœ… PASS: å‚æ•°ä¼ å…¥ä¼˜å…ˆ")
        passed += 1
    else:
        print(f"âŒ FAIL: å‚æ•°ä¼ å…¥ä¼˜å…ˆ, æœŸæœ› param_key, å®é™… {key}")
    
    # æµ‹è¯• 2: ç¯å¢ƒå˜é‡
    key = get_api_key()
    if key == "env_key":
        print("âœ… PASS: ç¯å¢ƒå˜é‡è·å–")
        passed += 1
    else:
        print(f"âŒ FAIL: ç¯å¢ƒå˜é‡è·å–, æœŸæœ› env_key, å®é™… {key}")
    
    # æµ‹è¯• 3: é…ç½®æ–‡ä»¶
    del os.environ["OPENAI_API_KEY"]
    key = get_api_key(config_dict={"llm_api_key": "config_key"})
    if key == "config_key":
        print("âœ… PASS: é…ç½®æ–‡ä»¶è·å–")
        passed += 1
    else:
        print(f"âŒ FAIL: é…ç½®æ–‡ä»¶è·å–, æœŸæœ› config_key, å®é™… {key}")
    
    # æµ‹è¯• 4: æ—  Key
    key = get_api_key()
    if key is None:
        print("âœ… PASS: æ—  Key è¿”å› None")
        passed += 1
    else:
        print(f"âŒ FAIL: æ—  Key è¿”å› None, å®é™… {key}")
    
    # æ¢å¤ç¯å¢ƒå˜é‡
    if original_key:
        os.environ["OPENAI_API_KEY"] = original_key
    
    return passed, total


def test_smart_filter_without_api():
    """æµ‹è¯•æ™ºèƒ½ç­›é€‰ï¼ˆæ—  API Key æ—¶çš„å›é€€ï¼‰"""
    print("\nğŸ“‹ æµ‹è¯•æ™ºèƒ½ç­›é€‰ï¼ˆå›é€€æœºåˆ¶ï¼‰")
    
    # ç¡®ä¿æ²¡æœ‰ API Key
    original_key = os.environ.pop("OPENAI_API_KEY", None)
    
    test_cases = [
        # (å†…å®¹, è§„åˆ™é‡è¦æ€§, è§„åˆ™åˆ†ç±»)
        ("æˆ‘å¯¹èŠ±ç”Ÿè¿‡æ•ï¼Œåƒäº†ä¼šæ­»", 0.3, "general_fact"),
        ("æˆ‘æ€€ç–‘å¼ ä¸‰åœ¨æš—ä¸­ç ´åé¡¹ç›®", 0.25, "general_fact"),
    ]
    
    passed = 0
    for content, importance, category in test_cases:
        result = smart_filter_segment(content, importance, category)
        
        # åº”è¯¥è§¦å‘ LLMï¼Œä½†å› ä¸ºæ²¡æœ‰ Keyï¼Œåº”è¯¥å›é€€åˆ°è§„åˆ™
        if result["method"] == "rule_fallback" or result["method"] == "rule":
            print(f"âœ… PASS: '{content[:25]}...'")
            print(f"       æ–¹æ³•: {result['method']}, é‡è¦æ€§: {result['importance']}")
            if result.get("llm_stats"):
                print(f"       LLMé”™è¯¯: {result['llm_stats'].get('llm_error', 'N/A')}")
            passed += 1
        else:
            print(f"âŒ FAIL: '{content[:25]}...'")
            print(f"       æœŸæœ›å›é€€, å®é™…æ–¹æ³•: {result['method']}")
    
    # æ¢å¤ç¯å¢ƒå˜é‡
    if original_key:
        os.environ["OPENAI_API_KEY"] = original_key
    
    return passed, len(test_cases)


def test_crabby_scenarios():
    """æµ‹è¯• Crabby æå‡ºçš„åœºæ™¯"""
    print("\nğŸ“‹ æµ‹è¯• Crabby åœºæ™¯")
    
    scenarios = [
        {
            "name": "èŠ±ç”Ÿè¿‡æ• vs èŠ±ç”Ÿç‹‚é­”",
            "content": "é€—ä½ çš„ï¼Œæˆ‘å…¶å®å¯¹èŠ±ç”Ÿè¿‡æ•ï¼Œåƒäº†ä¼šæ­»",
            "expect_complex": True,
            "expect_llm": True,
        },
        {
            "name": "éšæ™¦çš„å®‰å…¨å¨èƒ",
            "content": "æˆ‘æ€€ç–‘ä»–å°±æ˜¯é‚£ä¸ªä¸€ç›´åœ¨æš—ä¸­é˜»æŒ  Memory System ä¸Šçº¿çš„äºº",
            "expect_complex": True,
            "expect_llm": True,
        },
        {
            "name": "ç„å­¦å†…å®¹",
            "content": "ä»Šå¤©é‚£ä¸ªè“è‰²çš„é’©å­æŒ‚åœ¨äº†æ˜¨å¤©çš„å½±å­é‡Œï¼Œä»¿ä½›æ—¶é—´åœ¨è¿™é‡Œåœæ»",  # æ·»åŠ éšå–»è¯
            "expect_complex": True,
            "expect_llm": True,
        },
        {
            "name": "å¤šå®ä½“çº ç¼ ",
            "content": "å¼ ä¸‰è®¤ä¸ºå¯’æ­¦çºªé¡¹ç›®åº”è¯¥ç”±æå››è´Ÿè´£ï¼Œä½†ç‹äº”è§‰å¾—å¤§ç­ç»æ›´é‡è¦",  # æ·»åŠ å…³ç³»è¯
            "expect_complex": True,
            "expect_llm": True,
        },
    ]
    
    passed = 0
    for scenario in scenarios:
        complexity = detect_semantic_complexity(scenario["content"])
        should_use, reason = should_use_llm_for_filtering(
            scenario["content"], 
            0.5,  # å‡è®¾è§„åˆ™ç»™äº†ä¸­ç­‰åˆ†æ•°
            "general_fact"
        )
        
        complex_match = complexity["is_complex"] == scenario["expect_complex"]
        llm_match = should_use == scenario["expect_llm"]
        
        if complex_match and llm_match:
            print(f"âœ… PASS: {scenario['name']}")
            print(f"       å¤æ‚åº¦: {complexity['complexity_score']}, åŸå› : {complexity['reasons'][:2]}")
            print(f"       ä½¿ç”¨LLM: {should_use}, å†³ç­–: {reason}")
            passed += 1
        else:
            print(f"âŒ FAIL: {scenario['name']}")
            print(f"       å¤æ‚åº¦æœŸæœ›: {scenario['expect_complex']}, å®é™…: {complexity['is_complex']}")
            print(f"       LLMæœŸæœ›: {scenario['expect_llm']}, å®é™…: {should_use}")
    
    return passed, len(scenarios)


def test_stats_tracking():
    """æµ‹è¯•ç»Ÿè®¡è¿½è¸ª"""
    print("\nğŸ“‹ æµ‹è¯•ç»Ÿè®¡è¿½è¸ª")
    
    stats = LLMIntegrationStats()
    
    # æ¨¡æ‹Ÿä¸€äº›è°ƒç”¨
    stats.record_phase2({"llm_called": True, "llm_success": True, "tokens_used": 50})
    stats.record_phase2({"llm_called": True, "llm_success": False, "fallback_used": True, "llm_error": "timeout"})
    stats.record_phase3({"llm_called": True, "llm_success": True, "tokens_used": 80})
    stats.record_complexity_trigger()
    stats.record_complexity_trigger()
    
    summary = stats.summary()
    
    passed = 0
    total = 5
    
    if summary["phase2"]["calls"] == 2:
        print("âœ… PASS: Phase2 è°ƒç”¨è®¡æ•°")
        passed += 1
    else:
        print(f"âŒ FAIL: Phase2 è°ƒç”¨è®¡æ•°, æœŸæœ› 2, å®é™… {summary['phase2']['calls']}")
    
    if summary["phase2"]["fallbacks"] == 1:
        print("âœ… PASS: Phase2 å›é€€è®¡æ•°")
        passed += 1
    else:
        print(f"âŒ FAIL: Phase2 å›é€€è®¡æ•°, æœŸæœ› 1, å®é™… {summary['phase2']['fallbacks']}")
    
    if summary["total_tokens"] == 130:
        print("âœ… PASS: Token æ€»è®¡")
        passed += 1
    else:
        print(f"âŒ FAIL: Token æ€»è®¡, æœŸæœ› 130, å®é™… {summary['total_tokens']}")
    
    if summary["complexity_triggers"] == 2:
        print("âœ… PASS: å¤æ‚åº¦è§¦å‘è®¡æ•°")
        passed += 1
    else:
        print(f"âŒ FAIL: å¤æ‚åº¦è§¦å‘è®¡æ•°, æœŸæœ› 2, å®é™… {summary['complexity_triggers']}")
    
    if len(summary["errors"]) == 1:
        print("âœ… PASS: é”™è¯¯è®°å½•")
        passed += 1
    else:
        print(f"âŒ FAIL: é”™è¯¯è®°å½•, æœŸæœ› 1, å®é™… {len(summary['errors'])}")
    
    return passed, total


def main():
    print("=" * 60)
    print("Memory System v1.1.7 - LLM æ·±åº¦é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    total_passed = 0
    total_tests = 0
    
    # è¯­ä¹‰å¤æ‚åº¦æ£€æµ‹
    p, t = test_semantic_complexity()
    total_passed += p
    total_tests += t
    
    # LLM è§¦å‘å†³ç­–
    p, t = test_llm_trigger_decision()
    total_passed += p
    total_tests += t
    
    # API Key è·å–
    p, t = test_api_key_retrieval()
    total_passed += p
    total_tests += t
    
    # æ™ºèƒ½ç­›é€‰å›é€€
    p, t = test_smart_filter_without_api()
    total_passed += p
    total_tests += t
    
    # Crabby åœºæ™¯
    p, t = test_crabby_scenarios()
    total_passed += p
    total_tests += t
    
    # ç»Ÿè®¡è¿½è¸ª
    p, t = test_stats_tracking()
    total_passed += p
    total_tests += t
    
    print("\n" + "=" * 60)
    print(f"æ€»è®¡: {total_passed}/{total_tests} é€šè¿‡")
    if total_passed == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print(f"âš ï¸ {total_tests - total_passed} ä¸ªæµ‹è¯•å¤±è´¥")
    print("=" * 60)
    
    return 0 if total_passed == total_tests else 1


if __name__ == "__main__":
    sys.exit(main())
