#!/usr/bin/env python3
"""
Memory System v1.1.7 - LLM æ·±åº¦é›†æˆæ¨¡å—

æ ¸å¿ƒæ”¹è¿›ï¼š
1. è¯­ä¹‰å¤æ‚åº¦æ£€æµ‹ - è¯†åˆ«éœ€è¦ LLM å¤„ç†çš„å¤æ‚å†…å®¹
2. æ‰©å¤§ LLM è§¦å‘åŒºé—´ - ä¸ç¡®å®šåŒºé—´ä» 0.2~0.3 æ‰©å¤§åˆ° 0.2~0.5
3. LLM å¤±è´¥å›é€€æœºåˆ¶ - å¤±è´¥æ—¶å›é€€åˆ°è§„åˆ™ç»“æœï¼Œä¸ä¸¢å¼ƒ
4. API Key å¤šæºè·å– - ç¯å¢ƒå˜é‡ â†’ é…ç½®æ–‡ä»¶ â†’ å‚æ•°ä¼ å…¥
5. æ™ºèƒ½è°ƒç”¨ç­–ç•¥ - æ ¹æ®å†…å®¹å¤æ‚åº¦å†³å®šæ˜¯å¦è°ƒç”¨ LLM
"""

import os
import re
import json
from typing import Optional, Dict, Any, Tuple, List

# ============================================================
# é…ç½®
# ============================================================

LLM_INTEGRATION_CONFIG = {
    # LLM è§¦å‘ç­–ç•¥
    "trigger": {
        "high_confidence_threshold": 0.5,   # é«˜äºæ­¤å€¼ï¼Œè§„åˆ™ç›´æ¥æ¥å—ï¼ˆé™¤éè¯­ä¹‰å¤æ‚ï¼‰
        "low_confidence_threshold": 0.2,    # ä½äºæ­¤å€¼ï¼Œæ£€æŸ¥è¯­ä¹‰å¤æ‚åº¦
        "uncertain_range": (0.2, 0.5),      # ä¸ç¡®å®šåŒºé—´ï¼Œäº¤ç»™ LLM
        "force_llm_on_complex": True,       # å¤æ‚å†…å®¹å¼ºåˆ¶ LLM
    },
    
    # è¯­ä¹‰å¤æ‚åº¦æ£€æµ‹
    "complexity": {
        "min_length_for_check": 10,         # æœ€å°é•¿åº¦æ‰æ£€æµ‹å¤æ‚åº¦
        "entity_count_threshold": 3,        # å®ä½“æ•°é‡é˜ˆå€¼
        "relation_indicators": [            # å…³ç³»æŒ‡ç¤ºè¯
            "è®¤ä¸º", "è§‰å¾—", "æ€€ç–‘", "å¯èƒ½", "ä¹Ÿè®¸", "å¤§æ¦‚",
            "æ®è¯´", "å¬è¯´", "ä¼¼ä¹", "å¥½åƒ", "åº”è¯¥", "æˆ–è®¸",
            "å¦‚æœ", "å‡å¦‚", "é™¤é", "å°½ç®¡", "è™½ç„¶", "ä½†æ˜¯",
            "å› ä¸º", "æ‰€ä»¥", "å¯¼è‡´", "é€ æˆ", "å½±å“", "å…³ç³»",
        ],
        "negation_indicators": [            # å¦å®š/åè½¬æŒ‡ç¤ºè¯
            "ä¸æ˜¯", "æ²¡æœ‰", "å¹¶é", "å…¶å®", "å®é™…ä¸Š", "äº‹å®ä¸Š",
            "ç›¸å", "åè€Œ", "å´", "å€’æ˜¯", "æ°æ°",
        ],
        "metaphor_indicators": [            # éšå–»/æ¯”å–»æŒ‡ç¤ºè¯
            "åƒ", "å¦‚åŒ", "ä»¿ä½›", "å¥½æ¯”", "å°±åƒ", "çŠ¹å¦‚",
            "ç®€ç›´", "å ªæ¯”", "ä¸äºšäº", "èƒœè¿‡",
        ],
        "temporal_complexity": [            # æ—¶é—´å¤æ‚æ€§
            "ä¹‹å‰", "ä¹‹å", "åŒæ—¶", "æœŸé—´", "ç›´åˆ°", "è‡ªä»",
            "æ›¾ç»", "ä¸€ç›´", "æœ€è¿‘", "å°†æ¥", "è¿‡å»",
        ],
    },
    
    # å›é€€ç­–ç•¥
    "fallback": {
        "on_llm_failure": "rule",           # LLM å¤±è´¥æ—¶ï¼šruleï¼ˆå›é€€è§„åˆ™ï¼‰/ discardï¼ˆä¸¢å¼ƒï¼‰
        "on_parse_failure": "rule",         # è§£æå¤±è´¥æ—¶
        "retry_count": 1,                   # é‡è¯•æ¬¡æ•°
        "timeout_seconds": 30,              # è¶…æ—¶æ—¶é—´
    },
    
    # API é…ç½®
    "api": {
        "env_key": "OPENAI_API_KEY",        # ç¯å¢ƒå˜é‡å
        "config_key": "llm_api_key",        # é…ç½®æ–‡ä»¶é”®å
        "default_model": "gpt-4o-mini",     # é»˜è®¤æ¨¡å‹ï¼ˆä¾¿å®œä¸”å¿«ï¼‰
        "default_base_url": "https://api.openai.com/v1",
    },
}

# ============================================================
# è¯­ä¹‰å¤æ‚åº¦æ£€æµ‹
# ============================================================

def detect_semantic_complexity(content: str) -> Dict[str, Any]:
    """
    æ£€æµ‹å†…å®¹çš„è¯­ä¹‰å¤æ‚åº¦
    
    è¿”å›:
        {
            "is_complex": bool,          # æ˜¯å¦å¤æ‚
            "complexity_score": float,   # å¤æ‚åº¦åˆ†æ•° (0-1)
            "reasons": list,             # å¤æ‚åŸå› 
            "should_use_llm": bool,      # æ˜¯å¦åº”è¯¥ä½¿ç”¨ LLM
        }
    """
    config = LLM_INTEGRATION_CONFIG["complexity"]
    reasons = []
    score = 0.0
    
    # 1. é•¿åº¦æ£€æŸ¥
    if len(content) < config["min_length_for_check"]:
        return {
            "is_complex": False,
            "complexity_score": 0.0,
            "reasons": ["å†…å®¹å¤ªçŸ­"],
            "should_use_llm": False,
        }
    
    # 2. å…³ç³»æŒ‡ç¤ºè¯æ£€æµ‹
    relation_count = sum(1 for word in config["relation_indicators"] if word in content)
    if relation_count >= 1:  # é™ä½é˜ˆå€¼ï¼š1ä¸ªå…³ç³»è¯å°±åŠ åˆ†
        score += 0.2 + (relation_count - 1) * 0.1  # ç¬¬ä¸€ä¸ª 0.2ï¼Œä¹‹åæ¯ä¸ª +0.1
        reasons.append(f"åŒ…å« {relation_count} ä¸ªå…³ç³»æŒ‡ç¤ºè¯")
    
    # 3. å¦å®š/åè½¬æ£€æµ‹
    negation_count = sum(1 for word in config["negation_indicators"] if word in content)
    if negation_count >= 1:
        score += 0.25  # æé«˜å¦å®š/åè½¬çš„æƒé‡
        reasons.append(f"åŒ…å« {negation_count} ä¸ªå¦å®š/åè½¬è¯")
    
    # 4. éšå–»/æ¯”å–»æ£€æµ‹
    metaphor_count = sum(1 for word in config["metaphor_indicators"] if word in content)
    if metaphor_count >= 1:
        score += 0.25
        reasons.append(f"åŒ…å« {metaphor_count} ä¸ªéšå–»/æ¯”å–»è¯")
    
    # 5. æ—¶é—´å¤æ‚æ€§æ£€æµ‹
    temporal_count = sum(1 for word in config["temporal_complexity"] if word in content)
    if temporal_count >= 2:
        score += 0.15
        reasons.append(f"åŒ…å« {temporal_count} ä¸ªæ—¶é—´å¤æ‚è¯")
    
    # 6. å¤šå®ä½“æ£€æµ‹ï¼ˆç®€å•ç‰ˆï¼šæ£€æµ‹äººåæ¨¡å¼ï¼‰
    # ä¸­æ–‡äººåï¼š2-4ä¸ªæ±‰å­—
    # è‹±æ–‡äººåï¼šé¦–å­—æ¯å¤§å†™
    chinese_names = re.findall(r'[\u4e00-\u9fff]{2,4}(?:å…ˆç”Ÿ|å¥³å£«|åŒå­¦|è€å¸ˆ|åŒ»ç”Ÿ|æ•™æˆ)?', content)
    english_names = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?', content)
    entity_count = len(set(chinese_names)) + len(set(english_names))
    
    if entity_count >= config["entity_count_threshold"]:
        score += 0.25  # æé«˜å¤šå®ä½“çš„æƒé‡
        reasons.append(f"åŒ…å« {entity_count} ä¸ªå¯èƒ½çš„å®ä½“")
    
    # 7. å¥å­ç»“æ„å¤æ‚åº¦ï¼ˆç®€å•ç‰ˆï¼šæ£€æµ‹æ ‡ç‚¹ç¬¦å·å¯†åº¦ï¼‰
    punctuation_count = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]', content))
    if len(content) > 0:
        punct_density = punctuation_count / len(content)
        if punct_density > 0.1:  # æ ‡ç‚¹å¯†åº¦é«˜
            score += 0.1
            reasons.append("å¥å­ç»“æ„å¤æ‚")
    
    # 8. é—®å¥æ£€æµ‹
    if '?' in content or 'ï¼Ÿ' in content or content.endswith('å—') or content.endswith('å‘¢'):
        score += 0.1
        reasons.append("åŒ…å«ç–‘é—®")
    
    # å½’ä¸€åŒ–åˆ†æ•°
    score = min(score, 1.0)
    
    # åˆ¤æ–­æ˜¯å¦å¤æ‚
    is_complex = score >= 0.4
    
    # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ LLM
    should_use_llm = score >= 0.3  # ç¨å¾®å®½æ¾ä¸€ç‚¹
    
    return {
        "is_complex": is_complex,
        "complexity_score": round(score, 2),
        "reasons": reasons,
        "should_use_llm": should_use_llm,
    }


# ============================================================
# API Key è·å–
# ============================================================

def get_api_key(config_dict: Optional[Dict] = None, param_key: Optional[str] = None) -> Optional[str]:
    """
    å¤šæºè·å– API Key
    
    ä¼˜å…ˆçº§ï¼šå‚æ•°ä¼ å…¥ > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶
    
    å‚æ•°:
        config_dict: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        param_key: ç›´æ¥ä¼ å…¥çš„ API Keyï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        API Key æˆ– None
    """
    api_config = LLM_INTEGRATION_CONFIG["api"]
    
    # 1. å‚æ•°ä¼ å…¥ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    if param_key:
        return param_key
    
    # 2. ç¯å¢ƒå˜é‡
    env_key = os.environ.get(api_config["env_key"])
    if env_key:
        return env_key
    
    # 3. é…ç½®æ–‡ä»¶
    if config_dict and api_config["config_key"] in config_dict:
        return config_dict[api_config["config_key"]]
    
    return None


# ============================================================
# LLM è°ƒç”¨å°è£…
# ============================================================

def call_llm_with_fallback(
    prompt: str,
    system_prompt: str,
    fallback_result: Any,
    api_key: Optional[str] = None,
    config_dict: Optional[Dict] = None,
    max_tokens: int = 150,
) -> Tuple[Any, str, Dict]:
    """
    å¸¦å›é€€æœºåˆ¶çš„ LLM è°ƒç”¨
    
    å‚æ•°:
        prompt: ç”¨æˆ·æç¤º
        system_prompt: ç³»ç»Ÿæç¤º
        fallback_result: å¤±è´¥æ—¶çš„å›é€€ç»“æœ
        api_key: API Keyï¼ˆå¯é€‰ï¼‰
        config_dict: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        max_tokens: æœ€å¤§ token æ•°
    
    è¿”å›:
        (result, method, stats)
        - result: ç»“æœï¼ˆLLM ç»“æœæˆ–å›é€€ç»“æœï¼‰
        - method: æ–¹æ³•ï¼ˆ"llm" æˆ– "rule_fallback"ï¼‰
        - stats: ç»Ÿè®¡ä¿¡æ¯
    """
    stats = {
        "llm_called": False,
        "llm_success": False,
        "llm_error": None,
        "tokens_used": 0,
        "fallback_used": False,
    }
    
    # è·å– API Key
    key = get_api_key(config_dict, api_key)
    if not key:
        stats["llm_error"] = "API Key not found"
        stats["fallback_used"] = True
        return fallback_result, "rule_fallback", stats
    
    # è·å–é…ç½®
    api_config = LLM_INTEGRATION_CONFIG["api"]
    base_url = os.environ.get("OPENAI_BASE_URL", api_config["default_base_url"])
    model = os.environ.get("MEMORY_LLM_MODEL", api_config["default_model"])
    timeout = LLM_INTEGRATION_CONFIG["fallback"]["timeout_seconds"]
    
    stats["llm_called"] = True
    
    try:
        import requests
        
        headers = {
            "Authorization": f"Bearer {key}",
            "Content-Type": "application/json"
        }
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
        
        data = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": 0.3,
        }
        
        response = requests.post(
            f"{base_url}/chat/completions",
            headers=headers,
            json=data,
            timeout=timeout
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            usage = result.get("usage", {})
            
            stats["llm_success"] = True
            stats["tokens_used"] = usage.get("total_tokens", 0)
            
            return content.strip(), "llm", stats
        else:
            stats["llm_error"] = f"HTTP {response.status_code}: {response.text[:100]}"
            
    except requests.exceptions.Timeout:
        stats["llm_error"] = "Request timeout"
    except requests.exceptions.RequestException as e:
        stats["llm_error"] = f"Request error: {str(e)}"
    except Exception as e:
        stats["llm_error"] = f"Unexpected error: {str(e)}"
    
    # å›é€€
    stats["fallback_used"] = True
    return fallback_result, "rule_fallback", stats


# ============================================================
# æ™ºèƒ½ç­›é€‰å†³ç­–
# ============================================================

def should_use_llm_for_filtering(
    content: str,
    rule_importance: float,
    rule_category: str,
) -> Tuple[bool, str]:
    """
    å†³å®šæ˜¯å¦åº”è¯¥ä½¿ç”¨ LLM è¿›è¡Œç­›é€‰
    
    å‚æ•°:
        content: å†…å®¹
        rule_importance: è§„åˆ™è®¡ç®—çš„é‡è¦æ€§
        rule_category: è§„åˆ™è®¡ç®—çš„åˆ†ç±»
    
    è¿”å›:
        (should_use_llm, reason)
    """
    config = LLM_INTEGRATION_CONFIG["trigger"]
    
    # 1. æ£€æµ‹è¯­ä¹‰å¤æ‚åº¦
    complexity = detect_semantic_complexity(content)
    
    # 2. é«˜ç½®ä¿¡åº¦ + ç®€å•å†…å®¹ â†’ ä¿¡ä»»è§„åˆ™
    if rule_importance >= config["high_confidence_threshold"]:
        if not complexity["is_complex"]:
            return False, "high_confidence_simple"
        elif config["force_llm_on_complex"]:
            return True, f"high_confidence_but_complex: {complexity['reasons']}"
        else:
            return False, "high_confidence_complex_but_not_forced"
    
    # 3. ä½ç½®ä¿¡åº¦ â†’ æ£€æŸ¥å¤æ‚åº¦
    if rule_importance < config["low_confidence_threshold"]:
        if complexity["should_use_llm"]:
            return True, f"low_confidence_but_complex: {complexity['reasons']}"
        else:
            return False, "low_confidence_simple"
    
    # 4. ä¸ç¡®å®šåŒºé—´ â†’ ä½¿ç”¨ LLM
    low, high = config["uncertain_range"]
    if low <= rule_importance < high:
        return True, "uncertain_range"
    
    return False, "default_rule"


# ============================================================
# Phase 2 å¢å¼ºï¼šæ™ºèƒ½é‡è¦æ€§ç­›é€‰
# ============================================================

def smart_filter_segment(
    content: str,
    rule_importance: float,
    rule_category: str,
    api_key: Optional[str] = None,
    config_dict: Optional[Dict] = None,
) -> Dict[str, Any]:
    """
    æ™ºèƒ½ç­›é€‰å•ä¸ªç‰‡æ®µ
    
    ç»“åˆè§„åˆ™å’Œ LLMï¼Œè¿”å›æœ€ç»ˆçš„é‡è¦æ€§åˆ¤æ–­
    
    è¿”å›:
        {
            "importance": float,
            "category": str,
            "method": str,           # "rule" / "llm" / "rule_fallback"
            "complexity": dict,      # å¤æ‚åº¦åˆ†æ
            "llm_stats": dict,       # LLM è°ƒç”¨ç»Ÿè®¡
        }
    """
    # 1. å†³å®šæ˜¯å¦ä½¿ç”¨ LLM
    should_use, reason = should_use_llm_for_filtering(content, rule_importance, rule_category)
    
    result = {
        "importance": rule_importance,
        "category": rule_category,
        "method": "rule",
        "complexity": detect_semantic_complexity(content),
        "llm_stats": None,
        "decision_reason": reason,
    }
    
    if not should_use:
        return result
    
    # 2. æ„å»º LLM æç¤º
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªè®°å¿†é‡è¦æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯„ä¼°ç”¨æˆ·è¾“å…¥çš„é‡è¦æ€§ï¼ˆ0-1ï¼‰ï¼Œå¹¶åˆ†ç±»ã€‚

åˆ†ç±»æ ‡å‡†ï¼š
- identity_health_safety (0.9-1.0): èº«ä»½ã€å¥åº·ã€å®‰å…¨ã€è¿‡æ•ã€å¯†ç ã€å¯†é’¥
- preference_relation (0.7-0.9): åå¥½ã€å…³ç³»ã€æ€åº¦ã€è§‚ç‚¹
- project_task (0.5-0.7): é¡¹ç›®ã€ä»»åŠ¡ã€ç›®æ ‡ã€è®¡åˆ’
- general_fact (0.3-0.5): ä¸€èˆ¬äº‹å®ã€æè¿°
- temporary (0.1-0.3): ä¸´æ—¶ä¿¡æ¯ã€é—²èŠ

æ³¨æ„ï¼š
1. å¦‚æœå†…å®¹åŒ…å«å¥åº·é£é™©ï¼ˆå¦‚è¿‡æ•ï¼‰ï¼Œå¿…é¡»ç»™é«˜åˆ†
2. å¦‚æœå†…å®¹åŒ…å«å®‰å…¨ä¿¡æ¯ï¼ˆå¦‚å¯†ç ï¼‰ï¼Œå¿…é¡»ç»™é«˜åˆ†
3. å¦‚æœå†…å®¹åŒ…å«äººé™…å…³ç³»å˜åŒ–ï¼Œç»™ä¸­é«˜åˆ†
4. å¦‚æœå†…å®¹æ˜¯ç®€å•é—²èŠï¼Œç»™ä½åˆ†

è¿”å› JSONï¼š{"importance": 0.8, "category": "preference_relation", "reason": "ç®€çŸ­ç†ç”±"}"""

    prompt = f"è¯„ä¼°ä»¥ä¸‹å†…å®¹çš„é‡è¦æ€§ï¼š\n\n{content}\n\nè¿”å› JSONï¼š"
    
    # 3. è°ƒç”¨ LLMï¼ˆå¸¦å›é€€ï¼‰
    fallback_result = json.dumps({
        "importance": rule_importance,
        "category": rule_category,
        "reason": "LLM fallback to rule"
    })
    
    llm_response, method, stats = call_llm_with_fallback(
        prompt=prompt,
        system_prompt=system_prompt,
        fallback_result=fallback_result,
        api_key=api_key,
        config_dict=config_dict,
        max_tokens=100,
    )
    
    result["llm_stats"] = stats
    result["method"] = method
    
    # 4. è§£æ LLM ç»“æœ
    if method == "llm":
        try:
            # å°è¯•æå– JSON
            json_match = re.search(r'\{[^}]+\}', llm_response)
            if json_match:
                data = json.loads(json_match.group())
                result["importance"] = float(data.get("importance", rule_importance))
                result["category"] = data.get("category", rule_category)
                result["llm_reason"] = data.get("reason", "")
        except (json.JSONDecodeError, ValueError) as e:
            # è§£æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™
            result["method"] = "rule_fallback"
            result["parse_error"] = str(e)
    
    return result


# ============================================================
# Phase 3 å¢å¼ºï¼šæ™ºèƒ½å®ä½“æå–
# ============================================================

def smart_extract_entities(
    content: str,
    rule_entities: List[str],
    api_key: Optional[str] = None,
    config_dict: Optional[Dict] = None,
) -> Dict[str, Any]:
    """
    æ™ºèƒ½æå–å®ä½“
    
    å¦‚æœè§„åˆ™æå–çš„å®ä½“ä¸ºç©ºæˆ–å†…å®¹å¤æ‚ï¼Œä½¿ç”¨ LLM è¡¥å……
    
    è¿”å›:
        {
            "entities": list,
            "method": str,
            "llm_stats": dict,
        }
    """
    complexity = detect_semantic_complexity(content)
    
    result = {
        "entities": rule_entities,
        "method": "rule",
        "complexity": complexity,
        "llm_stats": None,
    }
    
    # å†³å®šæ˜¯å¦éœ€è¦ LLM
    need_llm = False
    reason = ""
    
    if not rule_entities:
        need_llm = True
        reason = "no_rule_entities"
    elif complexity["is_complex"] and len(rule_entities) < 2:
        need_llm = True
        reason = "complex_content_few_entities"
    
    if not need_llm:
        return result
    
    # æ„å»º LLM æç¤º
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®ä½“æå–ä¸“å®¶ã€‚ä»æ–‡æœ¬ä¸­æå–å…³é”®å®ä½“ã€‚

å®ä½“ç±»å‹ï¼š
- äººåï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰
- é¡¹ç›®å/äº§å“å
- ç»„ç»‡å/å…¬å¸å
- åœ°ç‚¹å
- ä¸“æœ‰åè¯

è¿”å› JSONï¼š{"entities": ["å®ä½“1", "å®ä½“2"], "types": {"å®ä½“1": "person", "å®ä½“2": "project"}}"""

    prompt = f"æå–ä»¥ä¸‹å†…å®¹ä¸­çš„å®ä½“ï¼š\n\n{content}\n\nè¿”å› JSONï¼š"
    
    fallback_result = json.dumps({"entities": rule_entities, "types": {}})
    
    llm_response, method, stats = call_llm_with_fallback(
        prompt=prompt,
        system_prompt=system_prompt,
        fallback_result=fallback_result,
        api_key=api_key,
        config_dict=config_dict,
        max_tokens=150,
    )
    
    result["llm_stats"] = stats
    result["method"] = method
    result["decision_reason"] = reason
    
    if method == "llm":
        try:
            json_match = re.search(r'\{[^}]+\}', llm_response)
            if json_match:
                data = json.loads(json_match.group())
                llm_entities = data.get("entities", [])
                # åˆå¹¶è§„åˆ™å’Œ LLM çš„å®ä½“
                all_entities = list(set(rule_entities + llm_entities))
                result["entities"] = all_entities
                result["entity_types"] = data.get("types", {})
        except (json.JSONDecodeError, ValueError) as e:
            result["method"] = "rule_fallback"
            result["parse_error"] = str(e)
    
    return result


# ============================================================
# ç»Ÿè®¡æ±‡æ€»
# ============================================================

class LLMIntegrationStats:
    """LLM é›†æˆç»Ÿè®¡"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.phase2_calls = 0
        self.phase2_successes = 0
        self.phase2_fallbacks = 0
        self.phase3_calls = 0
        self.phase3_successes = 0
        self.phase3_fallbacks = 0
        self.total_tokens = 0
        self.errors = []
        self.complexity_triggers = 0
    
    def record_phase2(self, stats: Dict):
        if stats and stats.get("llm_called"):
            self.phase2_calls += 1
            if stats.get("llm_success"):
                self.phase2_successes += 1
            if stats.get("fallback_used"):
                self.phase2_fallbacks += 1
            self.total_tokens += stats.get("tokens_used", 0)
            if stats.get("llm_error"):
                self.errors.append(f"Phase2: {stats['llm_error']}")
    
    def record_phase3(self, stats: Dict):
        if stats and stats.get("llm_called"):
            self.phase3_calls += 1
            if stats.get("llm_success"):
                self.phase3_successes += 1
            if stats.get("fallback_used"):
                self.phase3_fallbacks += 1
            self.total_tokens += stats.get("tokens_used", 0)
            if stats.get("llm_error"):
                self.errors.append(f"Phase3: {stats['llm_error']}")
    
    def record_complexity_trigger(self):
        self.complexity_triggers += 1
    
    def summary(self) -> Dict:
        return {
            "phase2": {
                "calls": self.phase2_calls,
                "successes": self.phase2_successes,
                "fallbacks": self.phase2_fallbacks,
            },
            "phase3": {
                "calls": self.phase3_calls,
                "successes": self.phase3_successes,
                "fallbacks": self.phase3_fallbacks,
            },
            "total_tokens": self.total_tokens,
            "complexity_triggers": self.complexity_triggers,
            "errors": self.errors[:5],  # åªä¿ç•™å‰ 5 ä¸ªé”™è¯¯
        }
    
    def print_summary(self):
        print("\nğŸ“Š LLM é›†æˆç»Ÿè®¡:")
        print(f"   Phase 2 (ç­›é€‰): {self.phase2_calls} è°ƒç”¨, {self.phase2_successes} æˆåŠŸ, {self.phase2_fallbacks} å›é€€")
        print(f"   Phase 3 (æå–): {self.phase3_calls} è°ƒç”¨, {self.phase3_successes} æˆåŠŸ, {self.phase3_fallbacks} å›é€€")
        print(f"   æ€» Token: {self.total_tokens}")
        print(f"   å¤æ‚åº¦è§¦å‘: {self.complexity_triggers} æ¬¡")
        if self.errors:
            print(f"   âš ï¸  é”™è¯¯: {len(self.errors)} ä¸ª")
            for err in self.errors[:3]:
                print(f"      - {err}")


# å…¨å±€ç»Ÿè®¡å®ä¾‹
INTEGRATION_STATS = LLMIntegrationStats()
