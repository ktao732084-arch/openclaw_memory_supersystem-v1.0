# 常见问题 FAQ

## 存储相关

### Q: 记忆内容怎么存储的？用了本地向量库吗？

**A: 没有用向量库。**

存储方式是 **JSONL + Markdown**：
- **Layer 2（长期记忆）**：JSONL 文件（facts.jsonl, beliefs.jsonl, summaries.jsonl）
- **Layer 3（原始日志）**：Markdown + JSONL 双格式
- **Layer 1（工作记忆）**：Markdown 快照，每次对话注入

检索方式：
- **精确匹配优先**：关键词索引、实体索引、时间线索引
- **语义搜索兜底**：调用 OpenClaw 内置的 memory_search

**为什么不用向量库？**
1. **轻量化**：不需要额外依赖（Chroma/Pinecone/Milvus）
2. **可读性**：JSONL 人类可直接查看和编辑
3. **成本控制**：向量化需要 embedding API 调用
4. **够用**：个人助手级别，精确索引 + 模型语义搜索已足够

---

### Q: 为什么不用数据库？

**A: 简单、可移植、够用。**

- **简单**：JSONL 是最轻量的结构化存储
- **可移植**：一个文件夹拷走就能用
- **可读**：人类直接打开就能看
- **够用**：个人助手级别不需要 SQLite/PostgreSQL

如果未来量大，可以迁移到数据库，架构兼容。

---

## 架构相关

### Q: 2000 tokens 够用吗？会不会太少？

**A: 够用。**

2000 是 **Layer 1（工作记忆）** 的预算，不是全部记忆。

- Layer 2 长期记忆**不限量**，按需检索
- 2000 tokens ≈ 1500 中文字，足够放：
  - 身份信息（~100 tokens）
  - 用户画像（~150 tokens）
  - Top 10-15 条重要记忆（~800 tokens）
  - 最近 24h 摘要（~200 tokens）

核心理念：**不是记得多，而是找得准**。

---

### Q: 和 RAG 有什么区别？

**A: 我们是"主动管理 + 被动检索"，RAG 是"被动检索"。**

| 特性 | RAG | Memory System |
|------|-----|---------------|
| 存储 | 向量库 | JSONL + 索引 |
| 检索 | 相似度匹配 | 精确 + 语义 |
| 整理 | ❌ 无 | ✅ Consolidation |
| 分类 | ❌ 不区分 | ✅ Fact/Belief/Summary |
| 遗忘 | ❌ 不会 | ✅ 自动衰减 |

---

### Q: Fact 和 Belief 怎么区分？谁来判断？

**A: LLM 在 Phase 3 提取时判断。**

- **Fact**：用户明确说过的（"我是医学生"）
- **Belief**：AI 推断的（用户经常问医学问题 → 可能对医学感兴趣）

置信度：
- Fact 默认 0.9-1.0
- Belief 根据证据强度 0.3-0.8

---

## Consolidation 相关

### Q: Consolidation 什么时候触发？会不会影响正常对话？

**A: 冷淡期触发，不影响对话。**

触发条件：
- 20分钟无消息
- 非活跃时段
- 无进行中的对话

类似人睡觉时整理记忆，不是边聊边整理。

---

### Q: 衰减公式怎么来的？为什么是这些参数？

**A: 基于艾宾浩斯遗忘曲线 + 实验调优。**

公式：`score = score × e^(-λ × days)`

参数设计：
| 类型 | λ | 半衰期 | 理由 |
|------|---|--------|------|
| Fact | 0.008 | ~87天 | 事实相对稳定 |
| Belief | 0.07 | ~10天 | 推断需快速验证 |
| Summary | 0.025 | ~28天 | 中等时效性 |
| Event | 0.15 | ~5天 | 原始事件快速衰减 |

用户可在 `config.json` 自定义调整。

---

### Q: 怎么处理冲突信息？比如用户改了说法

**A: Phase 4a 会做去重合并。**

- 新信息覆盖旧信息，保留时间戳
- 如果冲突明显，降低旧记录置信度
- 目前是简单策略，v1.1 计划加强冲突检测

---

## 性能相关

### Q: 性能瓶颈在哪？能支持多少条记忆？

**A: 瓶颈在 LLM 调用，本地操作很快。**

测试数据：
- 100 条记忆，Consolidation ~51ms
- 单条记忆添加 ~39ms

瓶颈分析：
- Phase 2/3 需要调用 LLM（主要成本）
- Phase 5/6 纯本地计算（很快）

预估支持 **1000-5000 条**无压力，再多需要优化索引。

---

### Q: 每次 Consolidation 消耗多少 Token？

**A: 约 1800 tokens/天。**

| Phase | Token 消耗 |
|-------|-----------|
| Phase 1 收集 | 0 |
| Phase 2 筛选 | ~700 |
| Phase 3 提取 | ~500 |
| Phase 4 分类 | ~400 |
| Phase 5 衰减 | 0 |
| Phase 6 索引 | 0 |
| Phase 7 快照 | ~200 |

通过"规则优先"策略，实际消耗可能更低。

---

## 扩展相关

### Q: 支持多用户吗？

**A: 当前设计是单用户。**

多用户需要：
- 每个用户独立的 memory/ 目录
- 用户隔离机制

架构上支持扩展，但 v1.0 没实现。

---

### Q: 开源协议是什么？可以商用吗？

**A: MIT License，可以商用。**

- 可以商用、修改、分发
- 唯一要求：保留版权声明

---

## 使用相关

### Q: 怎么调整衰减速率？

**A: 修改 `config.json`。**

```json
{
  "decay_rates": {
    "fact": 0.008,
    "belief": 0.07,
    "summary": 0.025
  }
}
```

数值越大，遗忘越快。

---

### Q: 怎么手动添加重要记忆？

**A: 使用 capture 命令。**

```bash
python memory.py capture "用户是医学生" --type fact --importance 0.95
```

---

### Q: 怎么查看当前记忆状态？

**A: 使用 status 或 stats 命令。**

```bash
python memory.py status  # 概览
python memory.py stats   # 详细统计
```
