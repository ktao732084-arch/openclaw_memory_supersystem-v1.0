# v1.3 优化方案：Crabby 的"减法"处方

> **来源**: Crabby 建议 (2026-02-12)
> **状态**: 待评估
> **目标**: 把"蒸汽朋克"系统改成"特种兵装备"

---

## 建议总览

| # | 建议 | 优先级 | 评估 | 理由 |
|---|------|--------|------|------|
| 1 | 高置信度直接注入 | P0 | ✅ 认同 | 省 LLM 调用，提速明显 |
| 2 | 单文件化 | P2 | ⚠️ 部分认同 | 收益不大，维护成本高 |
| 3 | mtime 替代 meta.json | P1 | ✅ 认同 | 更可靠，减少维护 |
| 4 | 锚点截取（前后10行） | P2 | ⚠️ 部分认同 | 先观察再说 |
| 5 | TTL 硬删除 | P1 | ✅ 认同 | 防止文件膨胀 |

---

## 详细分析

### 1. 逻辑收回：高置信度直接注入 ✅

**问题**：
> 现在的 router_search 还要过一遍 LLM 问它"这个结果好不好"，这是在烧钱买废话。

**Crabby 建议**：
- 硬编码过滤：QMD 返回的 score > 0.85 直接注入
- 跳过 LLM 的逻辑判断
- 高置信度的 facts.md 命中当成真理直接读

**评估**：✅ 认同
- 高分结果没必要再问 LLM
- 改动小，效果明显

**实现方案**：
```python
# router_search() 中
for qr in qmd_results:
    if qr.get('score', 0) > 0.85:
        # 高置信度直接注入，跳过后续 LLM 判断
        record['direct_inject'] = True
```

---

### 2. 结构坍缩：单文件化 ⚠️

**问题**：
> 脚本分成了 config, helpers, commands, llm_integration。这不是在写库，这只是个 Skill。

**Crabby 建议**：
- 把 v1_1_helpers.py 和 v1_1_config.py 全部合进 memory.py
- 减少 ImportError 风险
- 文件越少，运行越稳

**评估**：⚠️ 部分认同
- memory.py 已经 3200+ 行，再合并更难维护
- 现有 `V1_1_ENABLED` fallback 机制已经处理了 ImportError

**决策**：暂不执行，保持现状

---

### 3. 索引优化：mtime 替代 meta.json ✅

**问题**：
> 靠 meta.json 记脏数据。如果这个文件写坏了，整个系统就废了。

**Crabby 建议**：
- Lazy Indexing (懒加载索引)
- 用文件的 mtime (修改时间) 判断脏数据
- 搜索前读 memory/ 下文件的修改日期，新于索引文件就是脏的

**评估**：✅ 认同
- mtime 是操作系统原生支持，比自己维护 json 可靠
- 不用维护脆弱的计数器

**实现方案**：
```python
def is_index_stale(memory_dir):
    """检查索引是否过期（基于 mtime）"""
    index_mtime = (memory_dir / '.qmd/index').stat().st_mtime
    
    for jsonl in (memory_dir / 'layer2/active').glob('*.jsonl'):
        if jsonl.stat().st_mtime > index_mtime:
            return True  # 数据比索引新，需要更新
    
    return False
```

---

### 4. 锚点注入：只截取前后 10 行 ⚠️

**问题**：
> 注入是把整个块塞进去，会撑大上下文。

**Crabby 建议**：
- Context Peeking：只截取命中关键词前后 10 行
- 需要更多时用户主动要求

**评估**：⚠️ 部分认同
- 思路对，但实现复杂
- QMD 返回的 snippet 已经是截取过的
- 先观察实际 token 消耗，如果真的撑大上下文再优化

**决策**：暂缓，先观察

---

### 5. 记忆降噪：TTL 硬删除 ✅

**问题**：
> Consolidation 只是在反复重写，文件只会越来越大。

**Crabby 建议**：
- TTL (生存时间) 机制
- 7 天前的闲聊碎屑直接删了，不归档
- 只保留 LLM 提炼出来的 FACTS

**评估**：✅ 认同
- 现有衰减机制只是降权，没有"硬删除"
- 可以加一个 TTL 阈值

**实现方案**：
```python
# config.json
{
    "ttl": {
        "temporary": 7,      # 临时信息 7 天后删除
        "low_importance": 30, # 低重要性 30 天后删除
        "archive_threshold": 90  # 归档 90 天后删除
    }
}

# Phase 0 增强
def phase0_ttl_cleanup(memory_dir, config):
    """TTL 硬删除：超过生存时间的记忆直接删除"""
    ttl_config = config.get('ttl', {})
    deleted = 0
    
    for record in load_active_records():
        age_days = days_since(record['created'])
        importance = record.get('importance', 0.5)
        
        # 临时信息 7 天后删
        if importance < 0.3 and age_days > ttl_config.get('temporary', 7):
            delete_record(record)
            deleted += 1
    
    return deleted
```

---

## 实施计划

### Phase 1: 立即执行 (v1.3.0)

| 任务 | 改动量 | 收益 |
|------|--------|------|
| 高置信度直接注入 | ~20 行 | 省 LLM 调用 |
| mtime 脏数据检测 | ~30 行 | 更可靠 |

### Phase 2: 短期执行 (v1.3.1)

| 任务 | 改动量 | 收益 |
|------|--------|------|
| TTL 硬删除 | ~50 行 | 防止膨胀 |

### Phase 3: 观察后决定

| 任务 | 条件 |
|------|------|
| 锚点截取 | 如果 token 消耗过大 |
| 单文件化 | 如果 ImportError 频繁 |

---

## Crabby 的第一个任务

> 去把 `scripts/v1_1_7_llm_integration.py` 里的那个 `should_use_llm_for_filtering` 给删了。
> 改成一个简单的 `if qmd_score > threshold: inject_directly()`。

**评估**：这个建议的核心是对的，但不是删函数，而是在 router_search 中加一个快速路径：

```python
# 高置信度快速路径
if qmd_score > 0.85:
    return direct_inject(record)  # 跳过 LLM 判断
```

---

## 核心原则

Crabby 的哲学：
> "冷血的优化" = 敢砍掉不必要的"思考枷锁"

我们的平衡：
> 砍掉冗余判断，但保留必要的质量保证

---
*记录时间: 2026-02-12 05:07 UTC*
*来源: Crabby 建议*
*评估: Tkao*
