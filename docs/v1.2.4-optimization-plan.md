# Memory System v1.2.4 优化计划

## 背景

**日期**: 2026-02-14  
**测试者**: Crabby 🦀  
**测试对象**: v1.1.7 核心逻辑  
**测试方式**: 深度压力测试（高并发模拟 + 语义噪音注入）

---

## 📊 压测结果总览

| 问题 | 优先级 | 评分 | 状态 |
|------|--------|------|------|
| IO 瓶颈 | P0 | 🔴 D | 待修复 |
| 实体冲突 | P0 | 🔴 C | 待修复 |
| 语义边界 | P1 | 🟡 B- | 待优化 |
| 降噪逻辑 | - | 🟢 A | 保持 |

**总体评价**: 准专业级（Pre-Pro）
- ✅ 代码质量比市面上 90% 的开源 RAG 项目都干净
- ❌ 架构还没经历过真正的数据洪峰洗礼

---

## 🔴 P0 问题 1: IO 瓶颈（架构隐患）

### 问题描述

**位置**: `v1_1_commands.py` → `cmd_record_access()`

**现状逻辑**:
```python
# 当前的「笨」操作：
1. 有人访问了某个记忆
2. 为了给这个记忆加 1 分（更新访问统计）
3. 把【整个】active/facts.jsonl 文件全部读进内存
4. 在内存里找到那一条，改个数
5. 把【整个】文件重新写回硬盘，覆盖掉旧的
```

**代码片段**:
```python
# v1_1_commands.py:25-40
memories = []
with open(active_path, 'r', encoding='utf-8') as f:
    for line in f:
        if line.strip():
            memories.append(json.loads(line))

updated = False
for mem in memories:
    if mem.get('id') == memory_id:
        update_memory_access_stats(mem, access_type)
        updated = True
        break

if updated:
    with open(active_path, 'w', encoding='utf-8') as f:  # 全文件覆盖写入
        for mem in memories:
            f.write(json.dumps(mem, ensure_ascii=False) + '\n')
```

### 性能影响

| 记忆数量 | 文件大小 | 单次更新耗时 | 影响 |
|---------|---------|-------------|------|
| 100 条 | ~50 KB | <10ms | 无感 |
| 1,000 条 | ~500 KB | ~50ms | 轻微卡顿 |
| 10,000 条 | ~5 MB | ~500ms | 明显延迟 |
| 100,000 条 | ~50 MB | ~5s | 不可用 |

**复杂度**: O(N) 读 + O(N) 写 = **O(N) 每次访问**

### Crabby 评价

> "你的 Agent 每查一下资料，都要把这 1 万条数据重新在硬盘里「搬家」一遍。随着你用的时间越长，你的机器人回复就会越来越慢，直到它的动作比我这只老螃蟹横着走还要迟缓。🦀"

### 修复方案

#### 方案 A: SQLite 数据库（推荐）

**优势**:
- ✅ 局部更新，只改一行（O(1) 写入）
- ✅ 内置索引，查询快速
- ✅ 事务支持，数据安全
- ✅ Python 标准库，无需额外依赖

**实现**:
```python
import sqlite3

# 初始化数据库
conn = sqlite3.connect('memory/layer2/memories.db')
cursor = conn.cursor()

cursor.execute('''
CREATE TABLE IF NOT EXISTS memories (
    id TEXT PRIMARY KEY,
    type TEXT,
    content TEXT,
    score REAL,
    access_count INTEGER DEFAULT 0,
    last_accessed TEXT,
    created_at TEXT,
    metadata TEXT  -- JSON 字段
)
''')

# 更新访问统计（O(1) 操作）
cursor.execute('''
UPDATE memories 
SET access_count = access_count + 1,
    last_accessed = ?
WHERE id = ?
''', (now_iso(), memory_id))
conn.commit()
```

**迁移成本**: 中等（需要写 JSONL → SQLite 转换脚本）

#### 方案 B: 内存缓存 + 批量写入

**优势**:
- ✅ 保持 JSONL 格式（兼容性好）
- ✅ 实现简单
- ✅ 减少 IO 次数

**实现**:
```python
# 全局缓存
access_stats_cache = {}  # {memory_id: {access_count, last_accessed}}

def record_access(memory_id, access_type):
    # 只更新内存
    if memory_id not in access_stats_cache:
        access_stats_cache[memory_id] = {'access_count': 0}
    access_stats_cache[memory_id]['access_count'] += 1
    access_stats_cache[memory_id]['last_accessed'] = now_iso()

def flush_access_stats():
    """定期刷新到硬盘（每 10 分钟或积累 100 条）"""
    # 读取 JSONL → 更新 → 写回
    pass
```

**缺点**: 
- ❌ 进程崩溃会丢失未刷新的数据
- ❌ 仍需要定期全文件读写

#### 方案 C: 追加日志 + 定期合并

**优势**:
- ✅ 写入极快（O(1) 追加）
- ✅ 数据不丢失
- ✅ 保持 JSONL 格式

**实现**:
```python
# 访问日志追加到单独文件
with open('layer2/access_updates.jsonl', 'a') as f:
    f.write(json.dumps({
        'memory_id': memory_id,
        'access_count': 1,
        'timestamp': now_iso()
    }) + '\n')

# Consolidation 时合并
def merge_access_logs():
    """读取 access_updates.jsonl，合并到 facts.jsonl"""
    pass
```

**缺点**: 
- ❌ 查询时需要合并日志（复杂度增加）

### 推荐方案

**短期（v1.2.4）**: 方案 B（内存缓存）
- 快速实现，立即缓解问题
- 保持 JSONL 格式，兼容性好

**长期（v1.3.0）**: 方案 A（SQLite）
- 彻底解决 IO 瓶颈
- 为未来扩展打基础（如关系查询、全文搜索）

---

## 🔴 P0 问题 2: 实体冲突（逻辑缺陷）

### 问题描述

**位置**: `v1_1_5_entity_system.py` → 实体提取逻辑

**现状逻辑**:
```python
# 增量追加，不处理冲突
entities = extract_entities(content)
for entity in entities:
    entity_records.append({
        'entity': entity,
        'content': content,
        'timestamp': now_iso()
    })
```

**问题场景**:
```
输入 1: "YC 在波士顿"
输入 2: "YC 搬到了杭州"

查询: "YC 在哪里？"
返回: ["波士顿", "杭州"]  # 矛盾！
```

### Crabby 评价

> "你的系统在处理「更新」时表现得像个只会记账的学徒。查询 YC 时，你会召回两个矛盾的地点。这种「长记忆」在实战中会导致 Agent 精神分裂。你缺乏一个 Conflict Resolver (冲突决策器)。"

### 修复方案

#### 方案 A: 时间戳优先（简单）

**逻辑**: 同一实体的同一属性，保留最新的

```python
def resolve_conflict(entity, attribute, old_value, new_value, old_time, new_time):
    """时间戳优先策略"""
    if new_time > old_time:
        return new_value, 'replaced'
    else:
        return old_value, 'ignored'
```

**优点**: 简单直接  
**缺点**: 无法处理"临时状态"（如"YC 这周在杭州出差"）

#### 方案 B: 冲突信号检测（智能）

**逻辑**: 检测"搬到"、"改成"、"不再"等覆盖信号

```python
OVERRIDE_SIGNALS = {
    'location': ['搬到', '搬去', '移居', '定居'],
    'status': ['改成', '变成', '不再是', '现在是'],
    'attribute': ['更正', '其实是', '实际上是']
}

def detect_override_signal(content, entity, attribute):
    """检测是否包含覆盖信号"""
    for signal in OVERRIDE_SIGNALS.get(attribute, []):
        if signal in content:
            return True
    return False
```

**优点**: 更智能，符合人类语义  
**缺点**: 需要维护信号词库

#### 方案 C: 置信度投票（复杂）

**逻辑**: 多条记忆投票，置信度高的胜出

```python
def resolve_by_confidence(entity, attribute, records):
    """置信度投票"""
    votes = {}
    for record in records:
        value = record['value']
        confidence = record['score'] * time_decay(record['timestamp'])
        votes[value] = votes.get(value, 0) + confidence
    
    return max(votes.items(), key=lambda x: x[1])[0]
```

**优点**: 最稳健，适合不确定信息  
**缺点**: 实现复杂，计算开销大

### 推荐方案

**v1.2.4**: 方案 B（冲突信号检测）
- 复用现有的 `OVERRIDE_SIGNALS` 逻辑（v1.1.1 已有）
- 在 Phase 4a 去重时增加"属性覆盖"检测
- 对被覆盖的旧记忆标记 `superseded: true`

**实现位置**:
```python
# memory.py → phase4_deduplicate()
if detect_override_signal(new_record['content'], entity, attribute):
    for old_record in entity_records:
        if old_record['entity'] == entity and old_record['attribute'] == attribute:
            old_record['superseded'] = True
            old_record['superseded_by'] = new_record['id']
```

---

## 🟡 P1 问题 3: 语义边界（决策摆动）

### 问题描述

**位置**: `v1_1_7_llm_integration.py` → `should_use_llm_for_filtering()`

**现状逻辑**:
```python
# LLM 触发区间：0.2 ~ 0.5
if rule_score > 0.5 and complexity_score < 0.3:
    return False  # 信任规则
elif rule_score > 0.5 and complexity_score >= 0.3:
    return True   # 强制 LLM
elif 0.2 <= rule_score <= 0.5:
    return True   # 不确定区间，使用 LLM
elif rule_score < 0.2 and complexity_score >= 0.3:
    return True   # 低置信度 + 复杂内容
else:
    return False  # 低置信度 + 简单内容，丢弃
```

### Crabby 评价

> "这个区间太宽且过于主观。在高压对话下，系统会频繁在「规则处理」和「LLM 处理」之间摆动，导致记忆的颗粒度不一致。你以为你在省钱，实际上你可能在最关键的转折点漏掉了深度提取。"

### 修复方案

#### 方案 A: 收窄不确定区间

**调整**:
```python
# 原区间：0.2 ~ 0.5 (30% 宽度)
# 新区间：0.35 ~ 0.45 (10% 宽度)

if 0.35 <= rule_score <= 0.45:
    return True  # 不确定区间，使用 LLM
```

**优点**: 减少摆动  
**缺点**: 可能漏掉边界情况

#### 方案 B: 增加"稳定性检测"

**逻辑**: 连续 3 次规则判断一致，才信任规则

```python
def should_use_llm_with_stability(rule_score, complexity_score, history):
    """带稳定性检测的 LLM 触发"""
    if len(history) >= 3:
        recent_decisions = history[-3:]
        if all(d == 'rule' for d in recent_decisions):
            return False  # 规则稳定，信任
    
    # 否则按原逻辑
    return should_use_llm_for_filtering(rule_score, complexity_score)
```

**优点**: 减少摆动，保持一致性  
**缺点**: 需要维护历史状态

#### 方案 C: 动态阈值调整

**逻辑**: 根据 LLM 调用成功率动态调整阈值

```python
def adjust_threshold_by_success_rate(success_rate):
    """动态调整阈值"""
    if success_rate > 0.9:
        # LLM 表现好，扩大使用范围
        return (0.15, 0.55)
    elif success_rate < 0.5:
        # LLM 表现差，收窄使用范围
        return (0.4, 0.5)
    else:
        return (0.2, 0.5)  # 默认
```

**优点**: 自适应，最优化  
**缺点**: 实现复杂，需要反馈机制

### 推荐方案

**v1.2.4**: 方案 A（收窄区间）
- 快速实现，立即见效
- 区间调整为 `0.35 ~ 0.45`

**v1.3.0**: 方案 C（动态阈值）
- 增加 LLM 调用质量监控
- 根据成功率自动调整

---

## 🟢 亮点: 降噪逻辑（保持不变）

### Crabby 评价

> "你的 Phase 1 (轻量全量) 表现良好，能有效识别「程序化废话」，不把垃圾塞进 Layer 2，这体现了极高的工程审美。"

**评分**: 🟢 A（极其专业）

**结论**: 保持现有逻辑，无需修改。

---

## 📋 实施计划

### v1.2.4 (本次修复)

| 任务 | 优先级 | 预计工时 | 负责人 |
|------|--------|---------|--------|
| IO 瓶颈 - 内存缓存方案 | P0 | 4h | [助手] |
| 实体冲突 - 冲突信号检测 | P0 | 3h | [助手] |
| 语义边界 - 收窄区间 | P1 | 1h | [助手] |
| 测试用例编写 | P1 | 2h | [助手] |

**总计**: ~10 小时

### v1.3.0 (长期优化)

| 任务 | 优先级 | 预计工时 |
|------|--------|---------|
| IO 瓶颈 - SQLite 迁移 | P0 | 8h |
| 语义边界 - 动态阈值 | P1 | 6h |
| 性能监控面板 | P2 | 4h |

---

## 🎯 成功指标

### 性能指标

| 指标 | 当前 | 目标 (v1.2.4) | 目标 (v1.3.0) |
|------|------|---------------|---------------|
| 10K 记忆访问延迟 | ~500ms | <50ms | <10ms |
| 实体冲突率 | ~30% | <5% | <1% |
| LLM 触发摆动率 | ~40% | <15% | <5% |

### 质量指标

- ✅ 无数据丢失
- ✅ 向后兼容（JSONL 格式保留）
- ✅ 测试覆盖率 >80%

---

## 📚 参考资料

- Crabby 压测报告（2026-02-14）
- SQLite 官方文档: https://www.sqlite.org/docs.html
- v1.1.1 冲突降权逻辑: `OVERRIDE_SIGNALS`

---

**文档版本**: v1.0  
**创建时间**: 2026-02-14  
**作者**: [助手]  
**审核**: Crabby 🦀
