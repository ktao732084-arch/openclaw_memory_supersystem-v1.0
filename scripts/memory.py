#!/usr/bin/env python3
"""
Memory System v1.0 - ä¸‰å±‚è®°å¿†æ¶æ„ CLI
"""

import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

# ============================================================
# é…ç½®
# ============================================================

DEFAULT_CONFIG = {
    "version": "1.0",
    "decay_rates": {
        "fact": 0.008,
        "belief": 0.07,
        "summary": 0.025
    },
    "thresholds": {
        "archive": 0.05,
        "summary_trigger": 3
    },
    "token_budget": {
        "layer1_total": 2000
    },
    "consolidation": {
        "fallback_hours": 48
    }
}

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def get_memory_dir():
    """è·å–è®°å¿†ç³»ç»Ÿæ ¹ç›®å½•"""
    workspace = os.environ.get('WORKSPACE', os.getcwd())
    return Path(workspace) / 'memory'

def get_config():
    """è¯»å–é…ç½®"""
    config_path = get_memory_dir() / 'config.json'
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return DEFAULT_CONFIG

def save_config(config):
    """ä¿å­˜é…ç½®"""
    config_path = get_memory_dir() / 'config.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

def load_jsonl(path):
    """è¯»å– JSONL æ–‡ä»¶"""
    if not path.exists():
        return []
    records = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                records.append(json.loads(line))
    return records

def save_jsonl(path, records):
    """ä¿å­˜ JSONL æ–‡ä»¶"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        for record in records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')

def generate_id(prefix, content):
    """ç”Ÿæˆå”¯ä¸€ID"""
    date_str = datetime.now().strftime('%Y%m%d')
    hash_str = hashlib.md5(content.encode()).hexdigest()[:6]
    return f"{prefix}_{date_str}_{hash_str}"

def now_iso():
    """å½“å‰æ—¶é—´ ISO æ ¼å¼"""
    return datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')

# ============================================================
# åˆå§‹åŒ–å‘½ä»¤
# ============================================================

def cmd_init(args):
    """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿç›®å½•ç»“æ„"""
    memory_dir = get_memory_dir()
    
    # åˆ›å»ºç›®å½•ç»“æ„
    dirs = [
        'layer1',
        'layer2/active',
        'layer2/archive',
        'layer2/entities',
        'layer2/index',
        'state'
    ]
    
    for d in dirs:
        (memory_dir / d).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºé»˜è®¤é…ç½®
    config_path = memory_dir / 'config.json'
    if not config_path.exists():
        save_config(DEFAULT_CONFIG)
    
    # åˆ›å»ºç©ºçš„ JSONL æ–‡ä»¶
    jsonl_files = [
        'layer2/active/facts.jsonl',
        'layer2/active/beliefs.jsonl',
        'layer2/active/summaries.jsonl',
        'layer2/archive/facts.jsonl',
        'layer2/archive/beliefs.jsonl',
        'layer2/archive/summaries.jsonl'
    ]
    
    for f in jsonl_files:
        path = memory_dir / f
        if not path.exists():
            path.touch()
    
    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    index_files = {
        'layer2/index/keywords.json': {},
        'layer2/index/timeline.json': {},
        'layer2/index/relations.json': {},
        'layer2/entities/_index.json': {"entities": []}
    }
    
    for f, default in index_files.items():
        path = memory_dir / f
        if not path.exists():
            with open(path, 'w', encoding='utf-8') as fp:
                json.dump(default, fp, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
    state_files = {
        'state/consolidation.json': {
            "last_run": None,
            "last_success": None,
            "current_phase": None,
            "phase_data": {},
            "retry_count": 0
        },
        'state/rankings.json': {
            "updated": None,
            "rankings": []
        }
    }
    
    for f, default in state_files.items():
        path = memory_dir / f
        if not path.exists():
            with open(path, 'w', encoding='utf-8') as fp:
                json.dump(default, fp, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºåˆå§‹ Layer 1 å¿«ç…§
    snapshot_path = memory_dir / 'layer1/snapshot.md'
    if not snapshot_path.exists():
        snapshot_content = """# å·¥ä½œè®°å¿†å¿«ç…§
> ç”Ÿæˆæ—¶é—´: {time} | çŠ¶æ€: åˆå§‹åŒ–

## è¯´æ˜
è®°å¿†ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œå°šæ— è®°å¿†æ•°æ®ã€‚
æ‰§è¡Œ `memory.py consolidate` å¼€å§‹æ•´åˆè®°å¿†ã€‚
""".format(time=now_iso())
        with open(snapshot_path, 'w', encoding='utf-8') as f:
            f.write(snapshot_content)
    
    print("âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    print(f"   ç›®å½•: {memory_dir}")
    print(f"   é…ç½®: {memory_dir / 'config.json'}")

# ============================================================
# çŠ¶æ€å‘½ä»¤
# ============================================================

def cmd_status(args):
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œ: memory.py init")
        return
    
    # è¯»å–çŠ¶æ€
    state_path = memory_dir / 'state/consolidation.json'
    if state_path.exists():
        with open(state_path, 'r', encoding='utf-8') as f:
            state = json.load(f)
    else:
        state = {}
    
    # ç»Ÿè®¡è®°å¿†æ•°é‡
    active_facts = len(load_jsonl(memory_dir / 'layer2/active/facts.jsonl'))
    active_beliefs = len(load_jsonl(memory_dir / 'layer2/active/beliefs.jsonl'))
    active_summaries = len(load_jsonl(memory_dir / 'layer2/active/summaries.jsonl'))
    archive_facts = len(load_jsonl(memory_dir / 'layer2/archive/facts.jsonl'))
    archive_beliefs = len(load_jsonl(memory_dir / 'layer2/archive/beliefs.jsonl'))
    archive_summaries = len(load_jsonl(memory_dir / 'layer2/archive/summaries.jsonl'))
    
    active_total = active_facts + active_beliefs + active_summaries
    archive_total = archive_facts + archive_beliefs + archive_summaries
    
    print("ğŸ§  Memory System Status")
    print("=" * 40)
    print(f"ç›®å½•: {memory_dir}")
    print()
    print("ğŸ“Š è®°å¿†ç»Ÿè®¡")
    print(f"   æ´»è·ƒæ± : {active_total} æ¡")
    print(f"     - Facts: {active_facts}")
    print(f"     - Beliefs: {active_beliefs}")
    print(f"     - Summaries: {active_summaries}")
    print(f"   å½’æ¡£æ± : {archive_total} æ¡")
    print()
    print("â° Consolidation")
    print(f"   ä¸Šæ¬¡è¿è¡Œ: {state.get('last_run', 'ä»æœª')}")
    print(f"   ä¸Šæ¬¡æˆåŠŸ: {state.get('last_success', 'ä»æœª')}")
    print(f"   å½“å‰é˜¶æ®µ: {state.get('current_phase', 'æ— ')}")

def cmd_stats(args):
    """æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–")
        return
    
    # åŠ è½½æ‰€æœ‰è®°å¿†
    facts = load_jsonl(memory_dir / 'layer2/active/facts.jsonl')
    beliefs = load_jsonl(memory_dir / 'layer2/active/beliefs.jsonl')
    summaries = load_jsonl(memory_dir / 'layer2/active/summaries.jsonl')
    
    # æŒ‰é‡è¦æ€§åˆ†ç»„
    importance_groups = {
        'critical': 0,  # 0.9-1.0
        'high': 0,      # 0.7-0.9
        'medium': 0,    # 0.4-0.7
        'low': 0        # 0-0.4
    }
    
    all_records = facts + beliefs + summaries
    for r in all_records:
        imp = r.get('importance', 0.5)
        if imp >= 0.9:
            importance_groups['critical'] += 1
        elif imp >= 0.7:
            importance_groups['high'] += 1
        elif imp >= 0.4:
            importance_groups['medium'] += 1
        else:
            importance_groups['low'] += 1
    
    print("ğŸ“Š Memory System Stats")
    print("=" * 40)
    print(f"Total: {len(all_records)} memories")
    print()
    print("By Type:")
    print(f"  Facts: {len(facts)} ({len(facts)*100//max(len(all_records),1)}%)")
    print(f"  Beliefs: {len(beliefs)} ({len(beliefs)*100//max(len(all_records),1)}%)")
    print(f"  Summaries: {len(summaries)} ({len(summaries)*100//max(len(all_records),1)}%)")
    print()
    print("By Importance:")
    print(f"  Critical (0.9-1.0): {importance_groups['critical']}")
    print(f"  High (0.7-0.9): {importance_groups['high']}")
    print(f"  Medium (0.4-0.7): {importance_groups['medium']}")
    print(f"  Low (0-0.4): {importance_groups['low']}")

# ============================================================
# æ‰‹åŠ¨æ“ä½œå‘½ä»¤
# ============================================================

def cmd_capture(args):
    """æ‰‹åŠ¨æ·»åŠ è®°å¿†"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–")
        return
    
    content = args.content
    mem_type = args.type
    importance = args.importance
    
    # è¾“å…¥éªŒè¯
    if not content or not content.strip():
        print("âŒ é”™è¯¯: å†…å®¹ä¸èƒ½ä¸ºç©º")
        return
    
    # é™åˆ¶ importance åœ¨ 0-1 èŒƒå›´
    if importance < 0:
        importance = 0
        print("âš ï¸ è­¦å‘Š: importance å·²è°ƒæ•´ä¸º 0")
    elif importance > 1:
        importance = 1
        print("âš ï¸ è­¦å‘Š: importance å·²è°ƒæ•´ä¸º 1")
    entities = args.entities.split(',') if args.entities else []
    
    record = {
        "id": generate_id(mem_type[0], content),
        "content": content,
        "importance": importance,
        "score": importance,  # åˆå§‹ score = importance
        "entities": entities,
        "created": now_iso(),
        "source": "manual"
    }
    
    if mem_type == 'belief':
        record['confidence'] = args.confidence
    
    # è¿½åŠ åˆ°å¯¹åº”æ–‡ä»¶
    if mem_type == 'fact':
        path = memory_dir / 'layer2/active/facts.jsonl'
    elif mem_type == 'belief':
        path = memory_dir / 'layer2/active/beliefs.jsonl'
    else:
        path = memory_dir / 'layer2/active/summaries.jsonl'
    
    with open(path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(record, ensure_ascii=False) + '\n')
    
    print(f"âœ… è®°å¿†å·²æ·»åŠ : {record['id']}")
    print(f"   ç±»å‹: {mem_type}")
    print(f"   é‡è¦æ€§: {importance}")
    print(f"   å†…å®¹: {content[:50]}...")

def cmd_archive(args):
    """æ‰‹åŠ¨å½’æ¡£è®°å¿†"""
    memory_dir = get_memory_dir()
    memory_id = args.id
    
    # åœ¨æ´»è·ƒæ± ä¸­æŸ¥æ‰¾
    for mem_type in ['facts', 'beliefs', 'summaries']:
        active_path = memory_dir / f'layer2/active/{mem_type}.jsonl'
        archive_path = memory_dir / f'layer2/archive/{mem_type}.jsonl'
        
        records = load_jsonl(active_path)
        found = None
        remaining = []
        
        for r in records:
            if r.get('id') == memory_id:
                found = r
            else:
                remaining.append(r)
        
        if found:
            # ä¿å­˜å‰©ä½™è®°å½•
            save_jsonl(active_path, remaining)
            # è¿½åŠ åˆ°å½’æ¡£
            with open(archive_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(found, ensure_ascii=False) + '\n')
            print(f"âœ… å·²å½’æ¡£: {memory_id}")
            return
    
    print(f"âŒ æœªæ‰¾åˆ°è®°å¿†: {memory_id}")

# ============================================================
# Consolidation å‘½ä»¤
# ============================================================

def cmd_consolidate(args):
    """æ‰§è¡Œ Consolidation æµç¨‹"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œ: memory.py init")
        return
    
    config = get_config()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œ
    state_path = memory_dir / 'state/consolidation.json'
    with open(state_path, 'r', encoding='utf-8') as f:
        state = json.load(f)
    
    if not args.force and state.get('last_success'):
        last_success = datetime.fromisoformat(state['last_success'].replace('Z', '+00:00'))
        hours_since = (datetime.now(last_success.tzinfo) - last_success).total_seconds() / 3600
        fallback_hours = config['consolidation']['fallback_hours']
        
        if hours_since < 20:  # è‡³å°‘é—´éš” 20 å°æ—¶
            print(f"â­ï¸ è·³è¿‡: è·ç¦»ä¸Šæ¬¡æˆåŠŸä»… {hours_since:.1f} å°æ—¶")
            print(f"   ä½¿ç”¨ --force å¼ºåˆ¶æ‰§è¡Œ")
            return
    
    print("ğŸ§  å¼€å§‹ Consolidation...")
    print("=" * 40)
    
    # æ›´æ–°çŠ¶æ€
    state['last_run'] = now_iso()
    state['current_phase'] = 1
    with open(state_path, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2, ensure_ascii=False)
    
    try:
        # Phase 1: è½»é‡å…¨é‡
        if not args.phase or args.phase == 1:
            print("\nğŸ“‹ Phase 1: è½»é‡å…¨é‡ï¼ˆåˆ‡åˆ†ç‰‡æ®µï¼‰")
            print("   [æ¨¡æ‹Ÿ] è¯»å–ä»Šæ—¥å¯¹è¯ï¼Œåˆ‡åˆ†ä¸ºè¯­ä¹‰ç‰‡æ®µ")
            print("   âœ… å®Œæˆ")
        
        # Phase 2: é‡è¦æ€§ç­›é€‰
        if not args.phase or args.phase == 2:
            print("\nğŸ¯ Phase 2: é‡è¦æ€§ç­›é€‰")
            print("   [æ¨¡æ‹Ÿ] è°ƒç”¨æ¨¡å‹åˆ¤æ–­é‡è¦æ€§")
            print("   âœ… å®Œæˆ")
        
        # Phase 3: æ·±åº¦æå–
        if not args.phase or args.phase == 3:
            print("\nğŸ“ Phase 3: æ·±åº¦æå–")
            print("   [æ¨¡æ‹Ÿ] æå–ç»“æ„åŒ– facts/beliefs")
            print("   âœ… å®Œæˆ")
        
        # Phase 4: Layer 2 ç»´æŠ¤
        if not args.phase or args.phase == 4:
            print("\nğŸ”§ Phase 4: Layer 2 ç»´æŠ¤")
            print("   4a: Facts å»é‡åˆå¹¶")
            print("   4b: Beliefs éªŒè¯")
            print("   4c: Summaries ç”Ÿæˆ")
            print("   4d: Entities æ›´æ–°")
            print("   âœ… å®Œæˆ")
        
        # Phase 5: æƒé‡æ›´æ–°
        if not args.phase or args.phase == 5:
            print("\nâš–ï¸ Phase 5: æƒé‡æ›´æ–°")
            decay_rates = config['decay_rates']
            archive_threshold = config['thresholds']['archive']
            
            archived_count = 0
            for mem_type in ['facts', 'beliefs', 'summaries']:
                active_path = memory_dir / f'layer2/active/{mem_type}.jsonl'
                archive_path = memory_dir / f'layer2/archive/{mem_type}.jsonl'
                
                records = load_jsonl(active_path)
                remaining = []
                to_archive = []
                
                decay_rate = decay_rates.get(mem_type.rstrip('s'), 0.01)
                
                for r in records:
                    importance = r.get('importance', 0.5)
                    actual_decay = decay_rate * (1 - importance * 0.5)
                    r['score'] = r.get('score', importance) * (1 - actual_decay)
                    
                    if r['score'] < archive_threshold:
                        to_archive.append(r)
                        archived_count += 1
                    else:
                        remaining.append(r)
                
                save_jsonl(active_path, remaining)
                if to_archive:
                    existing = load_jsonl(archive_path)
                    save_jsonl(archive_path, existing + to_archive)
            
            print(f"   è¡°å‡å®Œæˆï¼Œå½’æ¡£ {archived_count} æ¡")
            print("   âœ… å®Œæˆ")
        
        # Phase 6: ç´¢å¼•æ›´æ–°
        if not args.phase or args.phase == 6:
            print("\nğŸ“‡ Phase 6: ç´¢å¼•æ›´æ–°")
            # é‡å»ºå…³é”®è¯ç´¢å¼•
            keywords_index = {}
            relations_index = {}
            
            for mem_type in ['facts', 'beliefs', 'summaries']:
                records = load_jsonl(memory_dir / f'layer2/active/{mem_type}.jsonl')
                for r in records:
                    # ç®€å•çš„å…³é”®è¯æå–
                    content = r.get('content', '')
                    words = content.replace('ï¼Œ', ' ').replace('ã€‚', ' ').split()
                    for word in words:
                        if len(word) >= 2:
                            if word not in keywords_index:
                                keywords_index[word] = []
                            keywords_index[word].append(r['id'])
                    
                    # å®ä½“å…³ç³»
                    for entity in r.get('entities', []):
                        if entity not in relations_index:
                            relations_index[entity] = {'facts': [], 'beliefs': [], 'summaries': []}
                        relations_index[entity][mem_type].append(r['id'])
            
            with open(memory_dir / 'layer2/index/keywords.json', 'w', encoding='utf-8') as f:
                json.dump(keywords_index, f, indent=2, ensure_ascii=False)
            with open(memory_dir / 'layer2/index/relations.json', 'w', encoding='utf-8') as f:
                json.dump(relations_index, f, indent=2, ensure_ascii=False)
            
            print("   âœ… å®Œæˆ")
        
        # Phase 7: Layer 1 å¿«ç…§
        if not args.phase or args.phase == 7:
            print("\nğŸ“¸ Phase 7: Layer 1 å¿«ç…§")
            
            # æ”¶é›†æ‰€æœ‰æ´»è·ƒè®°å¿†å¹¶æ’åº
            all_records = []
            for mem_type in ['facts', 'beliefs', 'summaries']:
                records = load_jsonl(memory_dir / f'layer2/active/{mem_type}.jsonl')
                for r in records:
                    r['_type'] = mem_type
                all_records.extend(records)
            
            # æŒ‰ score æ’åº
            all_records.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # ç»Ÿè®¡å„ç±»å‹æ•°é‡
            facts_count = len([r for r in all_records if r['_type'] == 'facts'])
            beliefs_count = len([r for r in all_records if r['_type'] == 'beliefs'])
            summaries_count = len([r for r in all_records if r['_type'] == 'summaries'])
            
            # æŒ‰é‡è¦æ€§åˆ†ç»„
            critical = [r for r in all_records if r.get('importance', 0) >= 0.9]
            high = [r for r in all_records if 0.7 <= r.get('importance', 0) < 0.9]
            
            # æå–å®ä½“ç»Ÿè®¡
            all_entities = set()
            for r in all_records:
                all_entities.update(r.get('entities', []))
            
            # ç”Ÿæˆå¢å¼ºç‰ˆå¿«ç…§
            snapshot = f"""# å·¥ä½œè®°å¿†å¿«ç…§
> ç”Ÿæˆæ—¶é—´: {now_iso()} | æ´»è·ƒè®°å¿†: {len(all_records)} | å®ä½“: {len(all_entities)}

---

## ğŸ”´ å…³é”®ä¿¡æ¯ (importance â‰¥ 0.9)
"""
            for r in critical[:5]:
                snapshot += f"- **{r.get('content', '')}**\n"
            if not critical:
                snapshot += "- (æ— )\n"
            
            snapshot += f"""
## ğŸŸ  é‡è¦ä¿¡æ¯ (importance 0.7-0.9)
"""
            for r in high[:5]:
                snapshot += f"- {r.get('content', '')}\n"
            if not high:
                snapshot += "- (æ— )\n"
            
            snapshot += f"""
## ğŸ“Š è®°å¿†æ’å (Top 15)
| # | Score | å†…å®¹ |
|---|-------|------|
"""
            for i, r in enumerate(all_records[:15]):
                score = r.get('score', 0)
                content_text = r.get('content', '')[:40]
                mem_type = r['_type'][0].upper()  # F/B/S
                snapshot += f"| {i+1} | {score:.2f} | [{mem_type}] {content_text} |\n"
            
            snapshot += f"""
## ğŸ·ï¸ å®ä½“ç´¢å¼•
"""
            for entity in sorted(all_entities)[:10]:
                related = len([r for r in all_records if entity in r.get('entities', [])])
                snapshot += f"- **{entity}**: {related} æ¡ç›¸å…³è®°å¿†\n"
            
            snapshot += f"""
## ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ
- **Facts**: {facts_count} æ¡ ({facts_count*100//max(len(all_records),1)}%)
- **Beliefs**: {beliefs_count} æ¡ ({beliefs_count*100//max(len(all_records),1)}%)
- **Summaries**: {summaries_count} æ¡ ({summaries_count*100//max(len(all_records),1)}%)
- **å…³é”®ä¿¡æ¯**: {len(critical)} æ¡
- **é‡è¦ä¿¡æ¯**: {len(high)} æ¡

---
*Memory System v1.0 | ä½¿ç”¨ memory_search æ£€ç´¢è¯¦ç»†ä¿¡æ¯*
"""
            
            with open(memory_dir / 'layer1/snapshot.md', 'w', encoding='utf-8') as f:
                f.write(snapshot)
            
            # ä¿å­˜æ’å
            rankings = [{'id': r['id'], 'score': r.get('score', 0)} for r in all_records[:50]]
            with open(memory_dir / 'state/rankings.json', 'w', encoding='utf-8') as f:
                json.dump({'updated': now_iso(), 'rankings': rankings}, f, indent=2, ensure_ascii=False)
            
            print("   âœ… å®Œæˆ")
        
        # æ›´æ–°æˆåŠŸçŠ¶æ€
        state['last_success'] = now_iso()
        state['current_phase'] = None
        state['retry_count'] = 0
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        
        print("\n" + "=" * 40)
        print("âœ… Consolidation å®Œæˆ!")
        
    except Exception as e:
        state['retry_count'] = state.get('retry_count', 0) + 1
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        print(f"\nâŒ Consolidation å¤±è´¥: {e}")
        raise

# ============================================================
# ç»´æŠ¤å‘½ä»¤
# ============================================================

def cmd_rebuild_index(args):
    """é‡å»ºç´¢å¼•"""
    memory_dir = get_memory_dir()
    print("ğŸ”„ é‡å»ºç´¢å¼•...")
    
    # è°ƒç”¨ Phase 6 é€»è¾‘
    args.phase = 6
    args.force = True
    cmd_consolidate(args)

def cmd_validate(args):
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    memory_dir = get_memory_dir()
    print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    errors = []
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    required_dirs = [
        'layer1', 'layer2/active', 'layer2/archive',
        'layer2/entities', 'layer2/index', 'state'
    ]
    for d in required_dirs:
        if not (memory_dir / d).exists():
            errors.append(f"ç¼ºå°‘ç›®å½•: {d}")
    
    # æ£€æŸ¥ JSONL æ–‡ä»¶æ ¼å¼
    for mem_type in ['facts', 'beliefs', 'summaries']:
        for pool in ['active', 'archive']:
            path = memory_dir / f'layer2/{pool}/{mem_type}.jsonl'
            if path.exists():
                try:
                    records = load_jsonl(path)
                    for i, r in enumerate(records):
                        if 'id' not in r:
                            errors.append(f"{path}:{i+1} ç¼ºå°‘ id å­—æ®µ")
                        if 'content' not in r:
                            errors.append(f"{path}:{i+1} ç¼ºå°‘ content å­—æ®µ")
                except Exception as e:
                    errors.append(f"{path} è§£æå¤±è´¥: {e}")
    
    if errors:
        print(f"âŒ å‘ç° {len(errors)} ä¸ªé—®é¢˜:")
        for e in errors[:10]:
            print(f"   - {e}")
        if len(errors) > 10:
            print(f"   ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé—®é¢˜")
    else:
        print("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")

# ============================================================
# ä¸»å…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description='Memory System v1.0 - ä¸‰å±‚è®°å¿†æ¶æ„ CLI',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # init
    parser_init = subparsers.add_parser('init', help='åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ')
    parser_init.set_defaults(func=cmd_init)
    
    # status
    parser_status = subparsers.add_parser('status', help='æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€')
    parser_status.set_defaults(func=cmd_status)
    
    # stats
    parser_stats = subparsers.add_parser('stats', help='æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡')
    parser_stats.set_defaults(func=cmd_stats)
    
    # capture
    parser_capture = subparsers.add_parser('capture', help='æ‰‹åŠ¨æ·»åŠ è®°å¿†')
    parser_capture.add_argument('content', help='è®°å¿†å†…å®¹')
    parser_capture.add_argument('--type', choices=['fact', 'belief', 'summary'], default='fact', help='è®°å¿†ç±»å‹')
    parser_capture.add_argument('--importance', type=float, default=0.5, help='é‡è¦æ€§ (0-1)')
    parser_capture.add_argument('--confidence', type=float, default=0.6, help='ç½®ä¿¡åº¦ (belief ä¸“ç”¨)')
    parser_capture.add_argument('--entities', default='', help='ç›¸å…³å®ä½“ï¼Œé€—å·åˆ†éš”')
    parser_capture.set_defaults(func=cmd_capture)
    
    # archive
    parser_archive = subparsers.add_parser('archive', help='æ‰‹åŠ¨å½’æ¡£è®°å¿†')
    parser_archive.add_argument('id', help='è®°å¿† ID')
    parser_archive.set_defaults(func=cmd_archive)
    
    # consolidate
    parser_consolidate = subparsers.add_parser('consolidate', help='æ‰§è¡Œ Consolidation')
    parser_consolidate.add_argument('--force', action='store_true', help='å¼ºåˆ¶æ‰§è¡Œ')
    parser_consolidate.add_argument('--phase', type=int, choices=[1,2,3,4,5,6,7], help='åªæ‰§è¡ŒæŒ‡å®šé˜¶æ®µ')
    parser_consolidate.set_defaults(func=cmd_consolidate)
    
    # rebuild-index
    parser_rebuild = subparsers.add_parser('rebuild-index', help='é‡å»ºç´¢å¼•')
    parser_rebuild.set_defaults(func=cmd_rebuild_index)
    
    # validate
    parser_validate = subparsers.add_parser('validate', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
    parser_validate.set_defaults(func=cmd_validate)
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    args.func(args)

if __name__ == '__main__':
    main()
