#!/usr/bin/env python3
"""
Memory System v1.0 - ä¸‰å±‚è®°å¿†æ¶æ„ CLI
"""

import os
import sys
import json
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

# ============================================================
# é…ç½®
# ============================================================

DEFAULT_CONFIG = {
    "version": "1.1.2",
    "decay_rates": {
        "fact": 0.008,
        "belief": 0.07,
        "summary": 0.025
    },
    "thresholds": {
        "archive": 0.05,
        "summary_trigger": 3
    },
    "token_budget": {
        "layer1_total": 2000
    },
    "consolidation": {
        "fallback_hours": 48
    },
    "conflict_detection": {
        "enabled": True,
        "penalty": 0.2
    }
}

# ============================================================
# é‡è¦æ€§è§„åˆ™é…ç½®
# ============================================================

IMPORTANCE_RULES = {
    # èº«ä»½/å¥åº·/å®‰å…¨ â†’ 1.0
    "identity_health_safety": {
        "keywords": ["è¿‡æ•", "ç–¾ç—…", "ç—…", "å¥åº·", "å®‰å…¨", "ç´§æ€¥", "å±é™©", 
                     "æˆ‘å«", "æˆ‘æ˜¯", "æˆ‘çš„åå­—", "èº«ä»½è¯", "ç”µè¯", "åœ°å€",
                     "å¯†ç ", "è´¦å·", "é“¶è¡Œ", "æ­»", "ç”Ÿå‘½"],
        "score": 1.0
    },
    # åå¥½/å…³ç³»/çŠ¶æ€å˜æ›´ â†’ 0.8
    "preference_relation_status": {
        "keywords": ["å–œæ¬¢", "è®¨åŒ", "çˆ±", "æ¨", "åå¥½", "ä¹ æƒ¯",
                     "æœ‹å‹", "å®¶äºº", "çˆ¶æ¯", "å¦ˆå¦ˆ", "çˆ¸çˆ¸", "å…„å¼Ÿ", "å§å¦¹",
                     "æ¢å·¥ä½œ", "æ¬å®¶", "æ¯•ä¸š", "ç»“å©š", "ç¦»å©š", "åˆ†æ‰‹",
                     "å¼€å§‹", "ç»“æŸ", "æ”¹å˜"],
        "score": 0.8
    },
    # é¡¹ç›®/ä»»åŠ¡/ç›®æ ‡ â†’ 0.7
    "project_task_goal": {
        "keywords": ["é¡¹ç›®", "ä»»åŠ¡", "ç›®æ ‡", "è®¡åˆ’", "deadline", "æˆªæ­¢",
                     "å¼€å‘", "è®¾è®¡", "å®ç°", "å®Œæˆ", "è¿›åº¦"],
        "score": 0.7
    },
    # ä¸€èˆ¬äº‹å® â†’ 0.5
    "general_fact": {
        "keywords": [],  # é»˜è®¤
        "score": 0.5
    },
    # ä¸´æ—¶ä¿¡æ¯ â†’ 0.2
    "temporary": {
        "keywords": ["ä»Šå¤©", "æ˜å¤©", "åˆšæ‰", "ä¸€ä¼šå„¿", "å¾…ä¼š", "é©¬ä¸Š",
                     "é¡ºä¾¿", "éšä¾¿", "æ— æ‰€è°“"],
        "score": 0.2
    }
}

# æ˜¾å¼ä¿¡å·åŠ æˆ
EXPLICIT_SIGNALS = {
    "boost_high": {
        "keywords": ["è®°ä½", "æ°¸è¿œè®°ä½", "ä¸€å®šè¦è®°ä½", "ä»¥åéƒ½", "æ°¸è¿œéƒ½"],
        "boost": 0.5
    },
    "boost_medium": {
        "keywords": ["é‡è¦", "å…³é”®", "å¿…é¡»", "ä¸€å®š"],
        "boost": 0.3
    },
    "boost_low": {
        "keywords": ["æ³¨æ„", "åˆ«å¿˜äº†"],
        "boost": 0.2
    },
    "reduce": {
        "keywords": ["é¡ºä¾¿è¯´ä¸€ä¸‹", "éšä¾¿é—®é—®", "ä¸é‡è¦", "æ— æ‰€è°“"],
        "boost": -0.2
    }
}

# å®ä½“è¯†åˆ«æ¨¡å¼ï¼ˆv1.1.2 æ”¹è¿›ï¼šæ”¯æŒæ­£åˆ™æ¨¡å¼ï¼‰
ENTITY_PATTERNS = {
    "person": {
        "fixed": ["æˆ‘", "ä½ ", "ä»–", "å¥¹", "ç”¨æˆ·", "Ktao", "Tkao"],
        "patterns": [
            r"[A-Z][a-z]+",  # è‹±æ–‡äººåï¼šJohn, Maryï¼ˆç§»é™¤\bï¼‰
        ]
    },
    "project": {
        "fixed": ["é¡¹ç›®", "ç³»ç»Ÿ", "å·¥å…·", "åº”ç”¨", "App"],
        "patterns": [
            r"é¡¹ç›®_\d+",  # é¡¹ç›®_1, é¡¹ç›®_25
            r"[A-Z][a-zA-Z0-9-]+",  # OpenClaw, Memory-Systemï¼ˆç§»é™¤\bï¼‰
        ]
    },
    "location": {
        "fixed": ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "å¹¿å·", "æ­å·", "æ²³å—", "éƒ‘å·"],
        "patterns": [
            r"åŸå¸‚_\d+",  # åŸå¸‚_1, åŸå¸‚_50
            r"åœ°ç‚¹_\d+",  # åœ°ç‚¹_1, åœ°ç‚¹_50
        ]
    },
    "organization": {
        "fixed": ["å…¬å¸", "å­¦æ ¡", "å¤§å­¦", "åŒ»é™¢", "å›¢é˜Ÿ"],
        "patterns": [
            r"ç»„ç»‡_\d+",  # ç»„ç»‡_1, ç»„ç»‡_50
            r"å›¢é˜Ÿ_\d+",  # å›¢é˜Ÿ_1, å›¢é˜Ÿ_50
        ]
    }
}

# å†²çªè¦†ç›–ä¿¡å·ï¼ˆv1.1.1 æ–°å¢ï¼‰
OVERRIDE_SIGNALS = [
    "ä¸å†", "æ”¹æˆ", "æ¢æˆ", "æ¬åˆ°", "ç°åœ¨æ˜¯", "å·²ç»æ˜¯",
    "ä¸æ˜¯", "è€Œæ˜¯", "ä»", "åˆ°", "ä¿®æ­£", "æ›´æ­£", "å˜æˆ"
]

# å†²çªé™æƒç³»æ•°
CONFLICT_PENALTY = 0.2

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def get_memory_dir():
    """è·å–è®°å¿†ç³»ç»Ÿæ ¹ç›®å½•"""
    workspace = os.environ.get('WORKSPACE', os.getcwd())
    return Path(workspace) / 'memory'

def get_config():
    """è¯»å–é…ç½®"""
    config_path = get_memory_dir() / 'config.json'
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return DEFAULT_CONFIG

def save_config(config):
    """ä¿å­˜é…ç½®"""
    config_path = get_memory_dir() / 'config.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

def load_jsonl(path):
    """è¯»å– JSONL æ–‡ä»¶"""
    if not path.exists():
        return []
    records = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                records.append(json.loads(line))
    return records

def save_jsonl(path, records):
    """ä¿å­˜ JSONL æ–‡ä»¶"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        for record in records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')

def generate_id(prefix, content):
    """ç”Ÿæˆå”¯ä¸€ID"""
    date_str = datetime.now().strftime('%Y%m%d')
    hash_str = hashlib.md5(content.encode()).hexdigest()[:6]
    return f"{prefix}_{date_str}_{hash_str}"

def now_iso():
    """å½“å‰æ—¶é—´ ISO æ ¼å¼"""
    return datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')

def append_jsonl(path, record):
    """è¿½åŠ å•æ¡è®°å½•åˆ° JSONL æ–‡ä»¶"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(record, ensure_ascii=False) + '\n')

# ============================================================
# Phase 2: é‡è¦æ€§ç­›é€‰ - rule_filter()
# ============================================================

def calculate_importance(content):
    """
    åŸºäºè§„åˆ™è®¡ç®—å†…å®¹çš„é‡è¦æ€§åˆ†æ•°
    è¿”å›: (importance_score, matched_category)
    """
    content_lower = content.lower()
    
    # 1. æ£€æŸ¥å†…åœ¨é‡è¦æ€§ï¼ˆä»é«˜åˆ°ä½ï¼‰
    for category in ["identity_health_safety", "preference_relation_status", 
                     "project_task_goal", "temporary"]:
        rule = IMPORTANCE_RULES[category]
        for keyword in rule["keywords"]:
            if keyword in content or keyword in content_lower:
                base_score = rule["score"]
                break
        else:
            continue
        break
    else:
        # é»˜è®¤ä¸ºä¸€èˆ¬äº‹å®
        base_score = IMPORTANCE_RULES["general_fact"]["score"]
        category = "general_fact"
    
    # 2. æ£€æŸ¥æ˜¾å¼ä¿¡å·åŠ æˆ
    boost = 0
    for signal_type, signal_config in EXPLICIT_SIGNALS.items():
        for keyword in signal_config["keywords"]:
            if keyword in content:
                boost = max(boost, signal_config["boost"]) if signal_config["boost"] > 0 else min(boost, signal_config["boost"])
                break
    
    # 3. è®¡ç®—æœ€ç»ˆåˆ†æ•°
    final_score = min(1.0, max(0.0, base_score + boost))
    
    return final_score, category

def rule_filter(segments, threshold=0.3):
    """
    Phase 2: é‡è¦æ€§ç­›é€‰
    è¾“å…¥: è¯­ä¹‰ç‰‡æ®µåˆ—è¡¨
    è¾“å‡º: ç­›é€‰åçš„é‡è¦ç‰‡æ®µåˆ—è¡¨ï¼ˆå¸¦ importance æ ‡æ³¨ï¼‰
    
    è§„åˆ™ä¼˜å…ˆï¼Œæ— éœ€ LLM è°ƒç”¨
    """
    filtered = []
    
    for segment in segments:
        content = segment.get("content", "") if isinstance(segment, dict) else segment
        
        # è®¡ç®—é‡è¦æ€§
        importance, category = calculate_importance(content)
        
        # ç­›é€‰
        if importance >= threshold:
            result = {
                "content": content,
                "importance": importance,
                "category": category,
                "source": segment.get("source", "unknown") if isinstance(segment, dict) else "unknown"
            }
            filtered.append(result)
    
    return filtered

# ============================================================
# Phase 3: æ·±åº¦æå– - template_extract()
# ============================================================

def extract_entities(content):
    """ä»å†…å®¹ä¸­æå–å®ä½“ï¼ˆv1.1.2 æ”¹è¿›ï¼šæ”¯æŒæ­£åˆ™æ¨¡å¼ï¼‰"""
    import re
    entities = []
    matched_positions = set()  # è®°å½•å·²åŒ¹é…çš„ä½ç½®ï¼Œé¿å…é‡å¤
    
    for entity_type, config in ENTITY_PATTERNS.items():
        # 1. å›ºå®šè¯åŒ¹é…
        if "fixed" in config:
            for word in config["fixed"]:
                if word in content:
                    entities.append(word)
        
        # 2. æ­£åˆ™æ¨¡å¼åŒ¹é…
        if "patterns" in config:
            for pattern in config["patterns"]:
                for match in re.finditer(pattern, content):
                    matched_text = match.group()
                    start, end = match.span()
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸å·²åŒ¹é…ä½ç½®é‡å 
                    if not any(start < pos < end or pos == start for pos in matched_positions):
                        entities.append(matched_text)
                        # è®°å½•æ‰€æœ‰åŒ¹é…ä½ç½®
                        for i in range(start, end):
                            matched_positions.add(i)
    
    # å»é‡å¹¶è¿‡æ»¤
    entities = [e for e in set(entities) if e and len(e) > 1]
    
    # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆä¿ç•™é•¿å®ä½“
    entities_by_length = {}
    for e in entities:
        length = len(e)
        if length not in entities_by_length:
            entities_by_length[length] = []
        entities_by_length[length].append(e)
    
    # è¿‡æ»¤ï¼šå¦‚æœçŸ­å®ä½“æ˜¯é•¿å®ä½“çš„å­ä¸²ï¼Œç§»é™¤çŸ­å®ä½“
    final_entities = []
    sorted_entities = sorted(entities, key=len, reverse=True)
    
    for entity in sorted_entities:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–å®ä½“çš„å­ä¸²
        is_substring = False
        for other in final_entities:
            if entity in other and entity != other:
                is_substring = True
                break
        if not is_substring:
            final_entities.append(entity)
    
    return final_entities

def classify_memory_type(content, importance):
    """
    åˆ¤æ–­è®°å¿†ç±»å‹: fact / belief / summary
    """
    content_lower = content.lower()
    
    # æ¨æ–­æ€§è¯æ±‡ â†’ belief
    belief_indicators = ["å¯èƒ½", "ä¹Ÿè®¸", "å¤§æ¦‚", "åº”è¯¥", "ä¼¼ä¹", "çœ‹èµ·æ¥", 
                         "æˆ‘è§‰å¾—", "æˆ‘è®¤ä¸º", "æˆ‘çŒœ", "ä¼°è®¡", "probably", "maybe"]
    for indicator in belief_indicators:
        if indicator in content_lower:
            return "belief"
    
    # èšåˆæ€§è¯æ±‡ â†’ summary
    summary_indicators = ["æ€»ç»“", "ç»¼ä¸Š", "æ€»çš„æ¥è¯´", "æ¦‚æ‹¬", "æ•´ä½“ä¸Š"]
    for indicator in summary_indicators:
        if indicator in content_lower:
            return "summary"
    
    # é»˜è®¤ â†’ fact
    return "fact"

def template_extract(filtered_segments):
    """
    Phase 3: æ·±åº¦æå–
    å°†ç­›é€‰åçš„ç‰‡æ®µè½¬ä¸ºç»“æ„åŒ– facts/beliefs
    
    æ¨¡æ¿åŒ¹é…ä¼˜å…ˆï¼Œæ— éœ€ LLM è°ƒç”¨
    """
    extracted = {
        "facts": [],
        "beliefs": [],
        "summaries": []
    }
    
    for segment in filtered_segments:
        content = segment["content"]
        importance = segment["importance"]
        source = segment.get("source", "unknown")
        
        # åˆ¤æ–­ç±»å‹
        mem_type = classify_memory_type(content, importance)
        
        # æå–å®ä½“
        entities = extract_entities(content)
        
        # æ„å»ºè®°å½•
        record = {
            "id": generate_id(mem_type[0], content),
            "content": content,
            "importance": importance,
            "score": importance,  # åˆå§‹ score = importance
            "entities": entities,
            "created": now_iso(),
            "source": source
        }
        
        # belief éœ€è¦é¢å¤–å­—æ®µ
        if mem_type == "belief":
            record["confidence"] = 0.6  # é»˜è®¤ç½®ä¿¡åº¦
            record["basis"] = f"ä»å¯¹è¯æ¨æ–­: {content[:50]}..."
        
        # åˆ†ç±»å­˜å‚¨
        extracted[f"{mem_type}s"].append(record)
    
    return extracted

# ============================================================
# Phase 4a: Facts å»é‡åˆå¹¶
# ============================================================

def deduplicate_facts(new_facts, existing_facts):
    """
    Phase 4a: Facts å»é‡åˆå¹¶ + å†²çªæ£€æµ‹ï¼ˆv1.1.1ï¼‰
    - ç›¸åŒå®ä½“ + ç›¸ä¼¼å†…å®¹ â†’ åˆå¹¶ï¼Œä¿ç•™æ›´æ–°ç‰ˆæœ¬
    - æ£€æµ‹è¦†ç›–ä¿¡å· â†’ å¯¹æ—§è®°å¿†æ‰§è¡Œæƒ©ç½šæ€§é™æƒ
    - è¿”å›: (merged_facts, duplicate_count, downgraded_count)
    """
    merged = []
    duplicate_count = 0
    downgraded_count = 0
    
    # å»ºç«‹ç°æœ‰ facts çš„ç´¢å¼•ï¼ˆæŒ‰å®ä½“åˆ†ç»„ï¼‰
    existing_by_entity = {}
    for fact in existing_facts:
        for entity in fact.get("entities", []):
            if entity not in existing_by_entity:
                existing_by_entity[entity] = []
            existing_by_entity[entity].append(fact)
    
    for new_fact in new_facts:
        is_duplicate = False
        new_content = new_fact["content"].lower()
        new_entities = new_fact.get("entities", [])
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«è¦†ç›–ä¿¡å·
        has_override = any(signal in new_fact["content"] for signal in OVERRIDE_SIGNALS)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰ fact é‡å¤æˆ–å†²çª
        for entity in new_entities:
            if entity in existing_by_entity:
                for existing in existing_by_entity[entity]:
                    existing_content = existing["content"].lower()
                    
                    # è®¡ç®—å†…å®¹é‡å åº¦
                    new_words = set(new_content.split())
                    existing_words = set(existing_content.split())
                    overlap = len(new_words & existing_words)
                    overlap_ratio = overlap / max(len(new_words), len(existing_words), 1)
                    
                    # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼šåŒ…å«å…³ç³»æˆ–é«˜åº¦é‡å 
                    if (new_content in existing_content or 
                        existing_content in new_content or
                        overlap > 3):
                        
                        # å¦‚æœæ–°è®°å¿†åŒ…å«è¦†ç›–ä¿¡å·ï¼Œæ‰§è¡Œå†²çªé™æƒ
                        if has_override and overlap_ratio > 0.3:
                            # æƒ©ç½šæ€§é™æƒ
                            old_score = existing.get("score", existing.get("importance", 0.5))
                            existing["score"] = old_score * CONFLICT_PENALTY
                            existing["conflict_downgraded"] = True
                            existing["downgrade_reason"] = new_fact["id"]
                            existing["downgrade_at"] = now_iso()
                            downgraded_count += 1
                            # ä¸æ ‡è®°ä¸ºé‡å¤ï¼Œå…è®¸æ–°è®°å¿†åŠ å…¥
                        else:
                            # æ­£å¸¸å»é‡ï¼šæ›´æ–°ç°æœ‰è®°å½•ï¼ˆä¿ç•™æ›´é«˜ importanceï¼‰
                            if new_fact["importance"] > existing.get("importance", 0):
                                existing["content"] = new_fact["content"]
                                existing["importance"] = new_fact["importance"]
                                existing["score"] = max(existing.get("score", 0), new_fact["score"])
                            is_duplicate = True
                            duplicate_count += 1
                        break
            if is_duplicate:
                break
        
        if not is_duplicate:
            merged.append(new_fact)
    
    return merged, duplicate_count, downgraded_count

# ============================================================
# Phase 4b: Beliefs éªŒè¯ - code_verify_belief()
# ============================================================

def code_verify_belief(belief, facts):
    """
    Phase 4b: Beliefs éªŒè¯
    æ£€æŸ¥ belief æ˜¯å¦è¢« facts è¯å®
    
    è¿”å›: ("confirmed" | "contradicted" | "unchanged", updated_belief)
    """
    belief_content = belief["content"].lower()
    belief_entities = belief.get("entities", [])
    
    for fact in facts:
        fact_content = fact["content"].lower()
        fact_entities = fact.get("entities", [])
        
        # æ£€æŸ¥å®ä½“é‡å 
        entity_overlap = set(belief_entities) & set(fact_entities)
        if not entity_overlap:
            continue
        
        # æ£€æŸ¥å†…å®¹å…³ç³»
        # 1. è¯å®ï¼šfact åŒ…å« belief çš„æ ¸å¿ƒå†…å®¹
        belief_words = set(belief_content.split())
        fact_words = set(fact_content.split())
        overlap_ratio = len(belief_words & fact_words) / max(len(belief_words), 1)
        
        if overlap_ratio > 0.5:
            # è¢«è¯å® â†’ å‡çº§ä¸º fact
            upgraded = belief.copy()
            upgraded["id"] = generate_id("f", belief["content"])
            upgraded["confidence"] = 1.0  # å‡çº§ä¸ºç¡®å®š
            upgraded["verified_by"] = fact["id"]
            upgraded["verified_at"] = now_iso()
            return "confirmed", upgraded
        
        # 2. çŸ›ç›¾æ£€æµ‹ï¼ˆç®€å•ç‰ˆï¼šå¦å®šè¯ï¼‰
        negation_words = ["ä¸", "æ²¡", "æ— ", "é", "å¦", "åˆ«", "ä¸æ˜¯", "æ²¡æœ‰"]
        belief_has_neg = any(neg in belief_content for neg in negation_words)
        fact_has_neg = any(neg in fact_content for neg in negation_words)
        
        if belief_has_neg != fact_has_neg and overlap_ratio > 0.3:
            # å¯èƒ½çŸ›ç›¾ â†’ é™ä½ç½®ä¿¡åº¦
            updated = belief.copy()
            updated["confidence"] = max(0.1, belief.get("confidence", 0.6) - 0.3)
            updated["contradiction_hint"] = fact["id"]
            return "contradicted", updated
    
    return "unchanged", belief

# ============================================================
# Phase 4c: Summaries ç”Ÿæˆ
# ============================================================

def generate_summaries(facts, existing_summaries, trigger_count=3):
    """
    Phase 4c: Summaries ç”Ÿæˆ
    å½“åŒä¸€å®ä½“æœ‰ >= trigger_count ä¸ª facts æ—¶ï¼Œç”Ÿæˆæ‘˜è¦
    
    è¿”å›: æ–°ç”Ÿæˆçš„ summaries åˆ—è¡¨
    """
    new_summaries = []
    
    # æŒ‰å®ä½“åˆ†ç»„ facts
    facts_by_entity = {}
    for fact in facts:
        for entity in fact.get("entities", []):
            if entity not in facts_by_entity:
                facts_by_entity[entity] = []
            facts_by_entity[entity].append(fact)
    
    # æ£€æŸ¥å·²æœ‰æ‘˜è¦è¦†ç›–çš„å®ä½“
    summarized_entities = set()
    for summary in existing_summaries:
        summarized_entities.update(summary.get("entities", []))
    
    # ä¸ºç¬¦åˆæ¡ä»¶çš„å®ä½“ç”Ÿæˆæ‘˜è¦
    for entity, entity_facts in facts_by_entity.items():
        if len(entity_facts) >= trigger_count and entity not in summarized_entities:
            # æŒ‰é‡è¦æ€§æ’åºï¼Œå– top facts
            sorted_facts = sorted(entity_facts, key=lambda x: x.get("importance", 0), reverse=True)
            top_facts = sorted_facts[:5]
            
            # ç”Ÿæˆæ‘˜è¦å†…å®¹ï¼ˆç®€å•æ‹¼æ¥ï¼‰
            summary_content = f"å…³äº{entity}çš„ä¿¡æ¯: " + "; ".join([f["content"][:30] for f in top_facts])
            
            # è®¡ç®—æ‘˜è¦é‡è¦æ€§ï¼ˆå–å¹³å‡ï¼‰
            avg_importance = sum(f.get("importance", 0.5) for f in top_facts) / len(top_facts)
            
            summary = {
                "id": generate_id("s", summary_content),
                "content": summary_content,
                "importance": avg_importance,
                "score": avg_importance,
                "entities": [entity],
                "source_facts": [f["id"] for f in top_facts],
                "created": now_iso()
            }
            new_summaries.append(summary)
    
    return new_summaries

# ============================================================
# Phase 4d: Entities æ›´æ–°
# ============================================================

def update_entities(facts, beliefs, summaries, memory_dir):
    """
    Phase 4d: Entities æ›´æ–°
    ç»´æŠ¤å®ä½“æ¡£æ¡ˆ
    """
    entities_dir = memory_dir / 'layer2/entities'
    index_path = entities_dir / '_index.json'
    
    # åŠ è½½ç°æœ‰ç´¢å¼•
    if index_path.exists():
        with open(index_path, 'r', encoding='utf-8') as f:
            index = json.load(f)
    else:
        index = {"entities": []}
    
    # æ”¶é›†æ‰€æœ‰å®ä½“
    all_entities = set()
    entity_facts = {}
    entity_beliefs = {}
    entity_summaries = {}
    
    for fact in facts:
        for entity in fact.get("entities", []):
            all_entities.add(entity)
            if entity not in entity_facts:
                entity_facts[entity] = []
            entity_facts[entity].append(fact["id"])
    
    for belief in beliefs:
        for entity in belief.get("entities", []):
            all_entities.add(entity)
            if entity not in entity_beliefs:
                entity_beliefs[entity] = []
            entity_beliefs[entity].append(belief["id"])
    
    for summary in summaries:
        for entity in summary.get("entities", []):
            all_entities.add(entity)
            if entity not in entity_summaries:
                entity_summaries[entity] = []
            entity_summaries[entity].append(summary["id"])
    
    # æ›´æ–°æ¯ä¸ªå®ä½“çš„æ¡£æ¡ˆ
    updated_count = 0
    for entity in all_entities:
        entity_id = hashlib.md5(entity.encode()).hexdigest()[:8]
        entity_path = entities_dir / f'{entity_id}.json'
        
        entity_data = {
            "id": entity_id,
            "name": entity,
            "facts": entity_facts.get(entity, []),
            "beliefs": entity_beliefs.get(entity, []),
            "summaries": entity_summaries.get(entity, []),
            "updated": now_iso()
        }
        
        with open(entity_path, 'w', encoding='utf-8') as f:
            json.dump(entity_data, f, indent=2, ensure_ascii=False)
        
        # æ›´æ–°ç´¢å¼•
        if entity not in [e["name"] for e in index["entities"]]:
            index["entities"].append({
                "id": entity_id,
                "name": entity,
                "count": len(entity_facts.get(entity, [])) + len(entity_beliefs.get(entity, []))
            })
        
        updated_count += 1
    
    # ä¿å­˜ç´¢å¼•
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, indent=2, ensure_ascii=False)
    
    return updated_count

# ============================================================
# Router é€»è¾‘ - æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ
# ============================================================

# è§¦å‘æ¡ä»¶å…³é”®è¯
TRIGGER_KEYWORDS = {
    "layer0_explicit": ["ä½ è¿˜è®°å¾—", "å¸®æˆ‘å›å¿†", "ä¹‹å‰è¯´è¿‡", "ä¸Šæ¬¡æåˆ°", "æˆ‘å‘Šè¯‰è¿‡ä½ "],
    "layer0_time": ["ä¹‹å‰", "ä»¥å‰", "ä¸Šæ¬¡", "æ˜¨å¤©", "å‰å‡ å¤©", "é‚£æ—¶å€™", "å½“æ—¶"],
    "layer1_preference": ["æˆ‘å–œæ¬¢", "æˆ‘è®¨åŒ", "æˆ‘åå¥½", "æˆ‘ä¹ æƒ¯", "æˆ‘çˆ±", "æˆ‘æ¨"],
    "layer1_identity": ["æˆ‘æ˜¯", "æˆ‘å«", "æˆ‘çš„åå­—", "å…³äºæˆ‘"],
    "layer1_relation": ["æœ‹å‹", "å®¶äºº", "åŒäº‹", "çˆ¶æ¯", "å…„å¼Ÿ", "å§å¦¹"],
    "layer1_project": ["é¡¹ç›®", "ä»»åŠ¡", "è®¡åˆ’", "ç›®æ ‡", "è¿›åº¦"],
}

# æŸ¥è¯¢ç±»å‹é…ç½®
QUERY_CONFIG = {
    "precise": {
        "initial": 15,
        "rerank": 10,
        "final": 8
    },
    "topic": {
        "initial": 25,
        "rerank": 16,
        "final": 13
    },
    "broad": {
        "initial": 35,
        "rerank": 25,
        "final": 18
    }
}

# ä¼šè¯ç¼“å­˜
_session_cache = {}
_cache_ttl = 1800  # 30åˆ†é’Ÿ

def get_cache_key(query):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(query.encode()).hexdigest()[:12]

def get_cached_result(query):
    """è·å–ç¼“å­˜ç»“æœ"""
    key = get_cache_key(query)
    if key in _session_cache:
        entry = _session_cache[key]
        if datetime.utcnow().timestamp() - entry["time"] < _cache_ttl:
            return entry["result"]
        else:
            del _session_cache[key]
    return None

def set_cached_result(query, result):
    """è®¾ç½®ç¼“å­˜ç»“æœ"""
    key = get_cache_key(query)
    _session_cache[key] = {
        "time": datetime.utcnow().timestamp(),
        "result": result
    }

def detect_trigger_layer(query):
    """
    æ£€æµ‹æŸ¥è¯¢è§¦å‘çš„å±‚çº§
    è¿”å›: (layer, trigger_type, matched_keywords)
    """
    query_lower = query.lower()
    
    # Layer 0: æ˜¾å¼è¯·æ±‚æˆ–æ—¶é—´å¼•ç”¨
    for trigger_type in ["layer0_explicit", "layer0_time"]:
        keywords = TRIGGER_KEYWORDS[trigger_type]
        matched = [kw for kw in keywords if kw in query_lower]
        if matched:
            return 0, trigger_type, matched
    
    # Layer 1: åå¥½/èº«ä»½/å…³ç³»/é¡¹ç›®
    for trigger_type in ["layer1_preference", "layer1_identity", "layer1_relation", "layer1_project"]:
        keywords = TRIGGER_KEYWORDS[trigger_type]
        matched = [kw for kw in keywords if kw in query_lower]
        if matched:
            return 1, trigger_type, matched
    
    # Layer 2: é»˜è®¤ï¼ˆä»»åŠ¡ç±»å‹æ˜ å°„ï¼‰
    return 2, "default", []

def classify_query_type(query, trigger_layer):
    """
    åˆ†ç±»æŸ¥è¯¢ç±»å‹: precise / topic / broad
    """
    query_lower = query.lower()
    
    # ç²¾å‡†æŸ¥è¯¢ï¼šå…·ä½“é—®é¢˜ã€ç‰¹å®šå®ä½“
    precise_indicators = ["æ˜¯ä»€ä¹ˆ", "æ˜¯è°", "åœ¨å“ª", "ä»€ä¹ˆæ—¶å€™", "å¤šå°‘", "å…·ä½“"]
    if any(ind in query_lower for ind in precise_indicators) or trigger_layer == 0:
        return "precise"
    
    # å¹¿åº¦æŸ¥è¯¢ï¼šæ€»ç»“ã€æ¦‚è§ˆã€æ‰€æœ‰
    broad_indicators = ["æ‰€æœ‰", "å…¨éƒ¨", "æ€»ç»“", "æ¦‚æ‹¬", "åˆ—å‡º", "æœ‰å“ªäº›"]
    if any(ind in query_lower for ind in broad_indicators):
        return "broad"
    
    # é»˜è®¤ï¼šä¸»é¢˜æŸ¥è¯¢
    return "topic"

def keyword_search(query, memory_dir, limit=20):
    """
    åŸºäºå…³é”®è¯çš„æ£€ç´¢
    è¿”å›: [(memory_id, score, content), ...]
    """
    import re
    
    # åŠ è½½å…³é”®è¯ç´¢å¼•
    keywords_path = memory_dir / 'layer2/index/keywords.json'
    if not keywords_path.exists():
        return []
    
    with open(keywords_path, 'r', encoding='utf-8') as f:
        keywords_index = json.load(f)
    
    # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    query_words = set()
    segments = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰\[\]ã€ã€‘\s]+', query)
    for seg in segments:
        seg = seg.strip()
        if len(seg) >= 2:
            query_words.add(seg)
        # æå–2-4å­—å­ä¸²
        for i in range(len(seg)):
            for length in [2, 3, 4]:
                if i + length <= len(seg):
                    sub = seg[i:i+length]
                    if len(sub) >= 2:
                        query_words.add(sub)
    
    # è®¡ç®—æ¯ä¸ªè®°å¿†çš„åŒ¹é…åˆ†æ•°
    memory_scores = {}
    for word in query_words:
        if word in keywords_index:
            for mem_id in keywords_index[word]:
                if mem_id not in memory_scores:
                    memory_scores[mem_id] = 0
                memory_scores[mem_id] += 1
    
    # åŠ è½½è®°å¿†å†…å®¹
    results = []
    all_memories = {}
    for mem_type in ['facts', 'beliefs', 'summaries']:
        records = load_jsonl(memory_dir / f'layer2/active/{mem_type}.jsonl')
        for r in records:
            all_memories[r['id']] = r
    
    # æ’åºå¹¶è¿”å›
    sorted_ids = sorted(memory_scores.keys(), key=lambda x: memory_scores[x], reverse=True)
    for mem_id in sorted_ids[:limit]:
        if mem_id in all_memories:
            mem = all_memories[mem_id]
            results.append({
                "id": mem_id,
                "score": memory_scores[mem_id],
                "content": mem.get("content", ""),
                "importance": mem.get("importance", 0.5),
                "memory_score": mem.get("score", 0.5),
                "type": "fact" if mem_id.startswith("f_") else ("belief" if mem_id.startswith("b_") else "summary")
            })
    
    return results

def entity_search(query, memory_dir, limit=20):
    """
    åŸºäºå®ä½“çš„æ£€ç´¢
    è¿”å›: [(memory_id, score, content), ...]
    """
    # åŠ è½½å®ä½“ç´¢å¼•
    relations_path = memory_dir / 'layer2/index/relations.json'
    if not relations_path.exists():
        return []
    
    with open(relations_path, 'r', encoding='utf-8') as f:
        relations_index = json.load(f)
    
    # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«å·²çŸ¥å®ä½“
    matched_entities = []
    for entity in relations_index.keys():
        if entity in query:
            matched_entities.append(entity)
    
    if not matched_entities:
        return []
    
    # æ”¶é›†ç›¸å…³è®°å¿†
    memory_ids = set()
    for entity in matched_entities:
        entity_data = relations_index[entity]
        for mem_type in ['facts', 'beliefs', 'summaries']:
            memory_ids.update(entity_data.get(mem_type, []))
    
    # åŠ è½½è®°å¿†å†…å®¹
    results = []
    all_memories = {}
    for mem_type in ['facts', 'beliefs', 'summaries']:
        records = load_jsonl(memory_dir / f'layer2/active/{mem_type}.jsonl')
        for r in records:
            all_memories[r['id']] = r
    
    for mem_id in list(memory_ids)[:limit]:
        if mem_id in all_memories:
            mem = all_memories[mem_id]
            results.append({
                "id": mem_id,
                "score": len([e for e in matched_entities if e in mem.get("entities", [])]),
                "content": mem.get("content", ""),
                "importance": mem.get("importance", 0.5),
                "memory_score": mem.get("score", 0.5),
                "type": "fact" if mem_id.startswith("f_") else ("belief" if mem_id.startswith("b_") else "summary")
            })
    
    return results

def rerank_results(results, query, limit):
    """
    é‡æ’åºæ£€ç´¢ç»“æœ
    ç»¼åˆè€ƒè™‘: åŒ¹é…åˆ†æ•° + è®°å¿†é‡è¦æ€§ + è®°å¿†score
    """
    for r in results:
        # ç»¼åˆåˆ†æ•° = åŒ¹é…åˆ†æ•° * 0.4 + é‡è¦æ€§ * 0.3 + è®°å¿†score * 0.3
        r["final_score"] = (
            r.get("score", 0) * 0.4 +
            r.get("importance", 0.5) * 0.3 +
            r.get("memory_score", 0.5) * 0.3
        )
    
    # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
    results.sort(key=lambda x: x["final_score"], reverse=True)
    return results[:limit]

def format_injection(results, confidence_threshold_high=0.8, confidence_threshold_low=0.5):
    """
    æ ¼å¼åŒ–æ³¨å…¥ç»“æœ
    - é«˜ç½®ä¿¡åº¦(>0.8): ç›´æ¥æ³¨å…¥ï¼Œæ— æ ‡è®°
    - ä¸­ç½®ä¿¡åº¦(0.5-0.8): æ³¨å…¥ + æ¥æºæ ‡è®°
    - ä½ç½®ä¿¡åº¦(<0.5): ä»…æä¾›å¼•ç”¨è·¯å¾„
    """
    output = {
        "direct": [],      # ç›´æ¥æ³¨å…¥
        "marked": [],      # å¸¦æ ‡è®°æ³¨å…¥
        "reference": []    # ä»…å¼•ç”¨
    }
    
    for r in results:
        confidence = r.get("memory_score", 0.5)
        
        if confidence >= confidence_threshold_high:
            output["direct"].append({
                "content": r["content"],
                "type": r["type"]
            })
        elif confidence >= confidence_threshold_low:
            output["marked"].append({
                "content": r["content"],
                "type": r["type"],
                "source": r["id"]
            })
        else:
            output["reference"].append({
                "id": r["id"],
                "preview": r["content"][:50] + "..."
            })
    
    return output

def router_search(query, memory_dir=None):
    """
    Router ä¸»å…¥å£ï¼šæ™ºèƒ½æ£€ç´¢è®°å¿†
    
    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢
        memory_dir: è®°å¿†ç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        {
            "trigger_layer": 0/1/2,
            "trigger_type": str,
            "query_type": "precise"/"topic"/"broad",
            "results": [...],
            "injection": {...},
            "cached": bool
        }
    """
    if memory_dir is None:
        memory_dir = get_memory_dir()
    
    # æ£€æŸ¥ç¼“å­˜
    cached = get_cached_result(query)
    if cached:
        cached["cached"] = True
        return cached
    
    # 1. æ£€æµ‹è§¦å‘å±‚çº§
    trigger_layer, trigger_type, matched_keywords = detect_trigger_layer(query)
    
    # 2. åˆ†ç±»æŸ¥è¯¢ç±»å‹
    query_type = classify_query_type(query, trigger_layer)
    config = QUERY_CONFIG[query_type]
    
    # 3. å¤šè·¯æ£€ç´¢
    keyword_results = keyword_search(query, memory_dir, limit=config["initial"])
    entity_results = entity_search(query, memory_dir, limit=config["initial"])
    
    # 4. åˆå¹¶å»é‡
    seen_ids = set()
    merged_results = []
    for r in keyword_results + entity_results:
        if r["id"] not in seen_ids:
            seen_ids.add(r["id"])
            merged_results.append(r)
    
    # 5. é‡æ’åº
    reranked = rerank_results(merged_results, query, config["rerank"])
    
    # 6. æœ€ç»ˆç­›é€‰
    final_results = reranked[:config["final"]]
    
    # 7. æ ¼å¼åŒ–æ³¨å…¥
    injection = format_injection(final_results)
    
    # æ„å»ºç»“æœ
    result = {
        "trigger_layer": trigger_layer,
        "trigger_type": trigger_type,
        "matched_keywords": matched_keywords,
        "query_type": query_type,
        "results": final_results,
        "injection": injection,
        "stats": {
            "keyword_hits": len(keyword_results),
            "entity_hits": len(entity_results),
            "merged": len(merged_results),
            "final": len(final_results)
        },
        "cached": False
    }
    
    # ç¼“å­˜ç»“æœ
    set_cached_result(query, result)
    
    return result

def cmd_search(args):
    """æ‰§è¡Œè®°å¿†æ£€ç´¢"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–")
        return
    
    query = args.query
    result = router_search(query, memory_dir)
    
    print(f"ğŸ” æ£€ç´¢: {query}")
    print("=" * 50)
    print(f"è§¦å‘å±‚çº§: Layer {result['trigger_layer']} ({result['trigger_type']})")
    print(f"æŸ¥è¯¢ç±»å‹: {result['query_type']}")
    print(f"åŒ¹é…å…³é”®è¯: {result['matched_keywords']}")
    print(f"ç¼“å­˜å‘½ä¸­: {'æ˜¯' if result['cached'] else 'å¦'}")
    print()
    print(f"ğŸ“Š æ£€ç´¢ç»Ÿè®¡:")
    print(f"   å…³é”®è¯å‘½ä¸­: {result['stats']['keyword_hits']}")
    print(f"   å®ä½“å‘½ä¸­: {result['stats']['entity_hits']}")
    print(f"   åˆå¹¶å: {result['stats']['merged']}")
    print(f"   æœ€ç»ˆç»“æœ: {result['stats']['final']}")
    print()
    
    if result['results']:
        print("ğŸ“‹ æ£€ç´¢ç»“æœ:")
        for i, r in enumerate(result['results'][:10]):
            print(f"   {i+1}. [{r['type'][0].upper()}] {r['content'][:50]}...")
            print(f"      score={r['final_score']:.2f}, importance={r['importance']:.1f}")
    else:
        print("ğŸ“‹ æ— åŒ¹é…ç»“æœ")
    
    print()
    print("ğŸ’‰ æ³¨å…¥å»ºè®®:")
    inj = result['injection']
    print(f"   ç›´æ¥æ³¨å…¥: {len(inj['direct'])} æ¡")
    print(f"   å¸¦æ ‡è®°æ³¨å…¥: {len(inj['marked'])} æ¡")
    print(f"   ä»…å¼•ç”¨: {len(inj['reference'])} æ¡")
    
    if args.json:
        print()
        print("ğŸ“„ JSON è¾“å‡º:")
        print(json.dumps(result, indent=2, ensure_ascii=False))

# ============================================================
# åˆå§‹åŒ–å‘½ä»¤
# ============================================================

def cmd_init(args):
    """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿç›®å½•ç»“æ„"""
    memory_dir = get_memory_dir()
    
    # åˆ›å»ºç›®å½•ç»“æ„
    dirs = [
        'layer1',
        'layer2/active',
        'layer2/archive',
        'layer2/entities',
        'layer2/index',
        'state'
    ]
    
    for d in dirs:
        (memory_dir / d).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºé»˜è®¤é…ç½®
    config_path = memory_dir / 'config.json'
    if not config_path.exists():
        save_config(DEFAULT_CONFIG)
    
    # åˆ›å»ºç©ºçš„ JSONL æ–‡ä»¶
    jsonl_files = [
        'layer2/active/facts.jsonl',
        'layer2/active/beliefs.jsonl',
        'layer2/active/summaries.jsonl',
        'layer2/archive/facts.jsonl',
        'layer2/archive/beliefs.jsonl',
        'layer2/archive/summaries.jsonl'
    ]
    
    for f in jsonl_files:
        path = memory_dir / f
        if not path.exists():
            path.touch()
    
    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    index_files = {
        'layer2/index/keywords.json': {},
        'layer2/index/timeline.json': {},
        'layer2/index/relations.json': {},
        'layer2/entities/_index.json': {"entities": []}
    }
    
    for f, default in index_files.items():
        path = memory_dir / f
        if not path.exists():
            with open(path, 'w', encoding='utf-8') as fp:
                json.dump(default, fp, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
    state_files = {
        'state/consolidation.json': {
            "last_run": None,
            "last_success": None,
            "current_phase": None,
            "phase_data": {},
            "retry_count": 0
        },
        'state/rankings.json': {
            "updated": None,
            "rankings": []
        }
    }
    
    for f, default in state_files.items():
        path = memory_dir / f
        if not path.exists():
            with open(path, 'w', encoding='utf-8') as fp:
                json.dump(default, fp, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºåˆå§‹ Layer 1 å¿«ç…§
    snapshot_path = memory_dir / 'layer1/snapshot.md'
    if not snapshot_path.exists():
        snapshot_content = """# å·¥ä½œè®°å¿†å¿«ç…§
> ç”Ÿæˆæ—¶é—´: {time} | çŠ¶æ€: åˆå§‹åŒ–

## è¯´æ˜
è®°å¿†ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œå°šæ— è®°å¿†æ•°æ®ã€‚
æ‰§è¡Œ `memory.py consolidate` å¼€å§‹æ•´åˆè®°å¿†ã€‚
""".format(time=now_iso())
        with open(snapshot_path, 'w', encoding='utf-8') as f:
            f.write(snapshot_content)
    
    print("âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    print(f"   ç›®å½•: {memory_dir}")
    print(f"   é…ç½®: {memory_dir / 'config.json'}")

# ============================================================
# çŠ¶æ€å‘½ä»¤
# ============================================================

def cmd_status(args):
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œ: memory.py init")
        return
    
    # è¯»å–çŠ¶æ€
    state_path = memory_dir / 'state/consolidation.json'
    if state_path.exists():
        with open(state_path, 'r', encoding='utf-8') as f:
            state = json.load(f)
    else:
        state = {}
    
    # ç»Ÿè®¡è®°å¿†æ•°é‡
    active_facts = len(load_jsonl(memory_dir / 'layer2/active/facts.jsonl'))
    active_beliefs = len(load_jsonl(memory_dir / 'layer2/active/beliefs.jsonl'))
    active_summaries = len(load_jsonl(memory_dir / 'layer2/active/summaries.jsonl'))
    archive_facts = len(load_jsonl(memory_dir / 'layer2/archive/facts.jsonl'))
    archive_beliefs = len(load_jsonl(memory_dir / 'layer2/archive/beliefs.jsonl'))
    archive_summaries = len(load_jsonl(memory_dir / 'layer2/archive/summaries.jsonl'))
    
    active_total = active_facts + active_beliefs + active_summaries
    archive_total = archive_facts + archive_beliefs + archive_summaries
    
    print("ğŸ§  Memory System Status")
    print("=" * 40)
    print(f"ç›®å½•: {memory_dir}")
    print()
    print("ğŸ“Š è®°å¿†ç»Ÿè®¡")
    print(f"   æ´»è·ƒæ± : {active_total} æ¡")
    print(f"     - Facts: {active_facts}")
    print(f"     - Beliefs: {active_beliefs}")
    print(f"     - Summaries: {active_summaries}")
    print(f"   å½’æ¡£æ± : {archive_total} æ¡")
    print()
    print("â° Consolidation")
    print(f"   ä¸Šæ¬¡è¿è¡Œ: {state.get('last_run', 'ä»æœª')}")
    print(f"   ä¸Šæ¬¡æˆåŠŸ: {state.get('last_success', 'ä»æœª')}")
    print(f"   å½“å‰é˜¶æ®µ: {state.get('current_phase', 'æ— ')}")

def cmd_stats(args):
    """æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–")
        return
    
    # åŠ è½½æ‰€æœ‰è®°å¿†
    facts = load_jsonl(memory_dir / 'layer2/active/facts.jsonl')
    beliefs = load_jsonl(memory_dir / 'layer2/active/beliefs.jsonl')
    summaries = load_jsonl(memory_dir / 'layer2/active/summaries.jsonl')
    
    # æŒ‰é‡è¦æ€§åˆ†ç»„
    importance_groups = {
        'critical': 0,  # 0.9-1.0
        'high': 0,      # 0.7-0.9
        'medium': 0,    # 0.4-0.7
        'low': 0        # 0-0.4
    }
    
    all_records = facts + beliefs + summaries
    for r in all_records:
        imp = r.get('importance', 0.5)
        if imp >= 0.9:
            importance_groups['critical'] += 1
        elif imp >= 0.7:
            importance_groups['high'] += 1
        elif imp >= 0.4:
            importance_groups['medium'] += 1
        else:
            importance_groups['low'] += 1
    
    print("ğŸ“Š Memory System Stats")
    print("=" * 40)
    print(f"Total: {len(all_records)} memories")
    print()
    print("By Type:")
    print(f"  Facts: {len(facts)} ({len(facts)*100//max(len(all_records),1)}%)")
    print(f"  Beliefs: {len(beliefs)} ({len(beliefs)*100//max(len(all_records),1)}%)")
    print(f"  Summaries: {len(summaries)} ({len(summaries)*100//max(len(all_records),1)}%)")
    print()
    print("By Importance:")
    print(f"  Critical (0.9-1.0): {importance_groups['critical']}")
    print(f"  High (0.7-0.9): {importance_groups['high']}")
    print(f"  Medium (0.4-0.7): {importance_groups['medium']}")
    print(f"  Low (0-0.4): {importance_groups['low']}")

# ============================================================
# æ‰‹åŠ¨æ“ä½œå‘½ä»¤
# ============================================================

def cmd_capture(args):
    """æ‰‹åŠ¨æ·»åŠ è®°å¿†"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–")
        return
    
    content = args.content
    mem_type = args.type
    importance = args.importance
    
    # è¾“å…¥éªŒè¯
    if not content or not content.strip():
        print("âŒ é”™è¯¯: å†…å®¹ä¸èƒ½ä¸ºç©º")
        return
    
    # é™åˆ¶ importance åœ¨ 0-1 èŒƒå›´
    if importance < 0:
        importance = 0
        print("âš ï¸ è­¦å‘Š: importance å·²è°ƒæ•´ä¸º 0")
    elif importance > 1:
        importance = 1
        print("âš ï¸ è­¦å‘Š: importance å·²è°ƒæ•´ä¸º 1")
    entities = args.entities.split(',') if args.entities else []
    
    record = {
        "id": generate_id(mem_type[0], content),
        "content": content,
        "importance": importance,
        "score": importance,  # åˆå§‹ score = importance
        "entities": entities,
        "created": now_iso(),
        "source": "manual"
    }
    
    if mem_type == 'belief':
        record['confidence'] = args.confidence
    
    # è¿½åŠ åˆ°å¯¹åº”æ–‡ä»¶
    if mem_type == 'fact':
        path = memory_dir / 'layer2/active/facts.jsonl'
    elif mem_type == 'belief':
        path = memory_dir / 'layer2/active/beliefs.jsonl'
    else:
        path = memory_dir / 'layer2/active/summaries.jsonl'
    
    with open(path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(record, ensure_ascii=False) + '\n')
    
    print(f"âœ… è®°å¿†å·²æ·»åŠ : {record['id']}")
    print(f"   ç±»å‹: {mem_type}")
    print(f"   é‡è¦æ€§: {importance}")
    print(f"   å†…å®¹: {content[:50]}...")

def cmd_archive(args):
    """æ‰‹åŠ¨å½’æ¡£è®°å¿†"""
    memory_dir = get_memory_dir()
    memory_id = args.id
    
    # åœ¨æ´»è·ƒæ± ä¸­æŸ¥æ‰¾
    for mem_type in ['facts', 'beliefs', 'summaries']:
        active_path = memory_dir / f'layer2/active/{mem_type}.jsonl'
        archive_path = memory_dir / f'layer2/archive/{mem_type}.jsonl'
        
        records = load_jsonl(active_path)
        found = None
        remaining = []
        
        for r in records:
            if r.get('id') == memory_id:
                found = r
            else:
                remaining.append(r)
        
        if found:
            # ä¿å­˜å‰©ä½™è®°å½•
            save_jsonl(active_path, remaining)
            # è¿½åŠ åˆ°å½’æ¡£
            with open(archive_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(found, ensure_ascii=False) + '\n')
            print(f"âœ… å·²å½’æ¡£: {memory_id}")
            return
    
    print(f"âŒ æœªæ‰¾åˆ°è®°å¿†: {memory_id}")

# ============================================================
# Consolidation å‘½ä»¤
# ============================================================

def cmd_consolidate(args):
    """æ‰§è¡Œ Consolidation æµç¨‹"""
    memory_dir = get_memory_dir()
    
    if not memory_dir.exists():
        print("âŒ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œ: memory.py init")
        return
    
    config = get_config()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œ
    state_path = memory_dir / 'state/consolidation.json'
    with open(state_path, 'r', encoding='utf-8') as f:
        state = json.load(f)
    
    if not args.force and state.get('last_success'):
        last_success = datetime.fromisoformat(state['last_success'].replace('Z', '+00:00'))
        hours_since = (datetime.now(last_success.tzinfo) - last_success).total_seconds() / 3600
        fallback_hours = config['consolidation']['fallback_hours']
        
        if hours_since < 20:  # è‡³å°‘é—´éš” 20 å°æ—¶
            print(f"â­ï¸ è·³è¿‡: è·ç¦»ä¸Šæ¬¡æˆåŠŸä»… {hours_since:.1f} å°æ—¶")
            print(f"   ä½¿ç”¨ --force å¼ºåˆ¶æ‰§è¡Œ")
            return
    
    print("ğŸ§  å¼€å§‹ Consolidation...")
    print("=" * 40)
    
    # æ›´æ–°çŠ¶æ€
    state['last_run'] = now_iso()
    state['current_phase'] = 1
    with open(state_path, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2, ensure_ascii=False)
    
    try:
        # ç”¨äºå­˜å‚¨ä¸­é—´ç»“æœ
        phase_data = state.get('phase_data', {})
        
        # Phase 1: è½»é‡å…¨é‡ï¼ˆæ¨¡æ‹Ÿ - éœ€è¦æ¥å…¥ OpenClaw sessionï¼‰
        if not args.phase or args.phase == 1:
            print("\nğŸ“‹ Phase 1: è½»é‡å…¨é‡ï¼ˆåˆ‡åˆ†ç‰‡æ®µï¼‰")
            # TODO: æ¥å…¥ OpenClaw session æ•°æ®
            # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æˆ–ä» stdin è¯»å–
            if args.input:
                with open(args.input, 'r', encoding='utf-8') as f:
                    raw_text = f.read()
                # ç®€å•æŒ‰å¥å­åˆ‡åˆ†
                segments = []
                for line in raw_text.split('\n'):
                    line = line.strip()
                    if line and len(line) > 5:
                        segments.append({"content": line, "source": args.input})
                phase_data['segments'] = segments
                print(f"   ä»æ–‡ä»¶è¯»å– {len(segments)} ä¸ªç‰‡æ®µ")
            else:
                print("   [è·³è¿‡] æ— è¾“å…¥æ•°æ®ï¼Œä½¿ç”¨ --input æŒ‡å®šè¾“å…¥æ–‡ä»¶")
                phase_data['segments'] = []
            print("   âœ… å®Œæˆ")
        
        # Phase 2: é‡è¦æ€§ç­›é€‰
        if not args.phase or args.phase == 2:
            print("\nğŸ¯ Phase 2: é‡è¦æ€§ç­›é€‰")
            segments = phase_data.get('segments', [])
            if segments:
                filtered = rule_filter(segments, threshold=0.3)
                phase_data['filtered'] = filtered
                print(f"   è¾“å…¥: {len(segments)} ç‰‡æ®µ")
                print(f"   ç­›é€‰å: {len(filtered)} ç‰‡æ®µ (threshold=0.3)")
                for f in filtered[:3]:
                    print(f"     - [{f['importance']:.1f}] {f['content'][:40]}...")
            else:
                phase_data['filtered'] = []
                print("   [è·³è¿‡] æ— è¾“å…¥ç‰‡æ®µ")
            print("   âœ… å®Œæˆ")
        
        # Phase 3: æ·±åº¦æå–
        if not args.phase or args.phase == 3:
            print("\nğŸ“ Phase 3: æ·±åº¦æå–")
            filtered = phase_data.get('filtered', [])
            if filtered:
                extracted = template_extract(filtered)
                phase_data['extracted'] = extracted
                print(f"   æå–ç»“æœ:")
                print(f"     - Facts: {len(extracted['facts'])}")
                print(f"     - Beliefs: {len(extracted['beliefs'])}")
                print(f"     - Summaries: {len(extracted['summaries'])}")
            else:
                phase_data['extracted'] = {'facts': [], 'beliefs': [], 'summaries': []}
                print("   [è·³è¿‡] æ— ç­›é€‰ç‰‡æ®µ")
            print("   âœ… å®Œæˆ")
        
        # Phase 4: Layer 2 ç»´æŠ¤
        if not args.phase or args.phase == 4:
            print("\nğŸ”§ Phase 4: Layer 2 ç»´æŠ¤")
            extracted = phase_data.get('extracted', {'facts': [], 'beliefs': [], 'summaries': []})
            
            # åŠ è½½ç°æœ‰è®°å¿†
            existing_facts = load_jsonl(memory_dir / 'layer2/active/facts.jsonl')
            existing_beliefs = load_jsonl(memory_dir / 'layer2/active/beliefs.jsonl')
            existing_summaries = load_jsonl(memory_dir / 'layer2/active/summaries.jsonl')
            
            # 4a: Facts å»é‡åˆå¹¶
            print("   4a: Facts å»é‡åˆå¹¶ + å†²çªæ£€æµ‹")
            new_facts = extracted.get('facts', [])
            if new_facts:
                merged_facts, dup_count, downgrade_count = deduplicate_facts(new_facts, existing_facts)
                print(f"       æ–°å¢: {len(merged_facts)}, å»é‡: {dup_count}, é™æƒ: {downgrade_count}")
                # è¿½åŠ æ–° facts
                for fact in merged_facts:
                    append_jsonl(memory_dir / 'layer2/active/facts.jsonl', fact)
                # å¦‚æœæœ‰é™æƒï¼Œéœ€è¦é‡å†™ existing_facts
                if downgrade_count > 0:
                    save_jsonl(memory_dir / 'layer2/active/facts.jsonl', existing_facts)
            else:
                print("       [è·³è¿‡] æ— æ–° facts")
            
            # 4b: Beliefs éªŒè¯
            print("   4b: Beliefs éªŒè¯")
            new_beliefs = extracted.get('beliefs', [])
            all_facts = existing_facts + extracted.get('facts', [])
            confirmed_count = 0
            contradicted_count = 0
            
            for belief in new_beliefs:
                status, updated = code_verify_belief(belief, all_facts)
                if status == "confirmed":
                    # å‡çº§ä¸º fact
                    append_jsonl(memory_dir / 'layer2/active/facts.jsonl', updated)
                    confirmed_count += 1
                elif status == "contradicted":
                    # é™ä½ç½®ä¿¡åº¦åä¿å­˜
                    append_jsonl(memory_dir / 'layer2/active/beliefs.jsonl', updated)
                    contradicted_count += 1
                else:
                    # ä¿æŒä¸å˜
                    append_jsonl(memory_dir / 'layer2/active/beliefs.jsonl', belief)
            
            print(f"       è¯å®â†’å‡çº§: {confirmed_count}, çŸ›ç›¾â†’é™æƒ: {contradicted_count}")
            
            # 4c: Summaries ç”Ÿæˆ
            print("   4c: Summaries ç”Ÿæˆ")
            all_facts_now = load_jsonl(memory_dir / 'layer2/active/facts.jsonl')
            trigger_count = config['thresholds'].get('summary_trigger', 3)
            new_summaries = generate_summaries(all_facts_now, existing_summaries, trigger_count)
            if new_summaries:
                for summary in new_summaries:
                    append_jsonl(memory_dir / 'layer2/active/summaries.jsonl', summary)
                print(f"       ç”Ÿæˆ: {len(new_summaries)} æ¡æ–°æ‘˜è¦")
            else:
                print("       [è·³è¿‡] æ— éœ€ç”Ÿæˆæ‘˜è¦")
            
            # 4d: Entities æ›´æ–°
            print("   4d: Entities æ›´æ–°")
            all_facts_final = load_jsonl(memory_dir / 'layer2/active/facts.jsonl')
            all_beliefs_final = load_jsonl(memory_dir / 'layer2/active/beliefs.jsonl')
            all_summaries_final = load_jsonl(memory_dir / 'layer2/active/summaries.jsonl')
            entity_count = update_entities(all_facts_final, all_beliefs_final, all_summaries_final, memory_dir)
            print(f"       æ›´æ–°: {entity_count} ä¸ªå®ä½“æ¡£æ¡ˆ")
            
            print("   âœ… å®Œæˆ")
        
        # Phase 5: æƒé‡æ›´æ–°
        if not args.phase or args.phase == 5:
            print("\nâš–ï¸ Phase 5: æƒé‡æ›´æ–°")
            decay_rates = config['decay_rates']
            archive_threshold = config['thresholds']['archive']
            
            archived_count = 0
            for mem_type in ['facts', 'beliefs', 'summaries']:
                active_path = memory_dir / f'layer2/active/{mem_type}.jsonl'
                archive_path = memory_dir / f'layer2/archive/{mem_type}.jsonl'
                
                records = load_jsonl(active_path)
                remaining = []
                to_archive = []
                
                decay_rate = decay_rates.get(mem_type.rstrip('s'), 0.01)
                
                for r in records:
                    importance = r.get('importance', 0.5)
                    actual_decay = decay_rate * (1 - importance * 0.5)
                    r['score'] = r.get('score', importance) * (1 - actual_decay)
                    
                    if r['score'] < archive_threshold:
                        to_archive.append(r)
                        archived_count += 1
                    else:
                        remaining.append(r)
                
                save_jsonl(active_path, remaining)
                if to_archive:
                    existing = load_jsonl(archive_path)
                    save_jsonl(archive_path, existing + to_archive)
            
            print(f"   è¡°å‡å®Œæˆï¼Œå½’æ¡£ {archived_count} æ¡")
            print("   âœ… å®Œæˆ")
        
        # Phase 6: ç´¢å¼•æ›´æ–°
        if not args.phase or args.phase == 6:
            print("\nğŸ“‡ Phase 6: ç´¢å¼•æ›´æ–°")
            # é‡å»ºå…³é”®è¯ç´¢å¼•
            keywords_index = {}
            relations_index = {}
            
            # ä¸­æ–‡åˆ†è¯è¾…åŠ©å‡½æ•°
            def extract_keywords(text):
                """æå–å…³é”®è¯ï¼ˆæ”¹è¿›ç‰ˆï¼šä¿ç•™è¿å­—ç¬¦è¯ï¼‰"""
                import re
                keywords = set()
                
                # 1. ä¼˜å…ˆæå–è¿å­—ç¬¦è¯ï¼ˆmemory-system, v1.1, API-keyç­‰ï¼‰
                hyphen_words = re.findall(r'[a-zA-Z0-9][-a-zA-Z0-9.]+', text)
                for word in hyphen_words:
                    if len(word) > 1:
                        keywords.add(word.lower())
                
                # 2. æå–ä¸­æ–‡è¯ç»„ï¼ˆ2å­—ä»¥ä¸Šï¼‰
                chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,}', text)
                for word in chinese_words:
                    keywords.add(word)
                
                # 3. æå–çº¯è‹±æ–‡å•è¯ï¼ˆä¸å«è¿å­—ç¬¦çš„ï¼‰
                english_words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
                for word in english_words:
                    keywords.add(word.lower())
                
                return keywords
            
            for mem_type in ['facts', 'beliefs', 'summaries']:
                records = load_jsonl(memory_dir / f'layer2/active/{mem_type}.jsonl')
                for r in records:
                    # æ”¹è¿›çš„å…³é”®è¯æå–
                    content = r.get('content', '')
                    keywords = extract_keywords(content)
                    for word in keywords:
                        if word not in keywords_index:
                            keywords_index[word] = []
                        if r['id'] not in keywords_index[word]:
                            keywords_index[word].append(r['id'])
                    
                    # å®ä½“å…³ç³»
                    for entity in r.get('entities', []):
                        if entity not in relations_index:
                            relations_index[entity] = {'facts': [], 'beliefs': [], 'summaries': []}
                        relations_index[entity][mem_type].append(r['id'])
            
            with open(memory_dir / 'layer2/index/keywords.json', 'w', encoding='utf-8') as f:
                json.dump(keywords_index, f, indent=2, ensure_ascii=False)
            with open(memory_dir / 'layer2/index/relations.json', 'w', encoding='utf-8') as f:
                json.dump(relations_index, f, indent=2, ensure_ascii=False)
            
            print("   âœ… å®Œæˆ")
        
        # Phase 7: Layer 1 å¿«ç…§
        if not args.phase or args.phase == 7:
            print("\nğŸ“¸ Phase 7: Layer 1 å¿«ç…§")
            
            # æ”¶é›†æ‰€æœ‰æ´»è·ƒè®°å¿†å¹¶æ’åº
            all_records = []
            for mem_type in ['facts', 'beliefs', 'summaries']:
                records = load_jsonl(memory_dir / f'layer2/active/{mem_type}.jsonl')
                for r in records:
                    r['_type'] = mem_type
                all_records.extend(records)
            
            # æŒ‰ score æ’åº
            all_records.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # ç»Ÿè®¡å„ç±»å‹æ•°é‡
            facts_count = len([r for r in all_records if r['_type'] == 'facts'])
            beliefs_count = len([r for r in all_records if r['_type'] == 'beliefs'])
            summaries_count = len([r for r in all_records if r['_type'] == 'summaries'])
            
            # æŒ‰é‡è¦æ€§åˆ†ç»„
            critical = [r for r in all_records if r.get('importance', 0) >= 0.9]
            high = [r for r in all_records if 0.7 <= r.get('importance', 0) < 0.9]
            downgraded = [r for r in all_records if r.get('conflict_downgraded', False)]
            
            # æå–å®ä½“ç»Ÿè®¡
            all_entities = set()
            for r in all_records:
                all_entities.update(r.get('entities', []))
            
            # ç”Ÿæˆå¢å¼ºç‰ˆå¿«ç…§
            snapshot = f"""# å·¥ä½œè®°å¿†å¿«ç…§
> ç”Ÿæˆæ—¶é—´: {now_iso()} | æ´»è·ƒè®°å¿†: {len(all_records)} | å®ä½“: {len(all_entities)}

---

## ğŸ”´ å…³é”®ä¿¡æ¯ (importance â‰¥ 0.9)
"""
            for r in critical[:5]:
                snapshot += f"- **{r.get('content', '')}**\n"
            if not critical:
                snapshot += "- (æ— )\n"
            
            snapshot += f"""
## ğŸŸ  é‡è¦ä¿¡æ¯ (importance 0.7-0.9)
"""
            for r in high[:5]:
                snapshot += f"- {r.get('content', '')}\n"
            if not high:
                snapshot += "- (æ— )\n"
            
            # æ–°å¢ï¼šé™æƒè®°å¿†æ ‡æ³¨
            if downgraded:
                snapshot += f"""
## ğŸ“‰ å·²é™æƒè®°å¿† (å†²çªè¦†ç›–)
"""
                for r in downgraded[:5]:
                    content = r.get('content', '')[:40]
                    old_score = r.get('importance', 0.5)
                    new_score = r.get('score', 0)
                    snapshot += f"- ~~{content}~~ (Score: {old_score:.2f} â†’ {new_score:.2f})\n"
            
            snapshot += f"""
## ğŸ“Š è®°å¿†æ’å (Top 15)
| # | Score | å†…å®¹ |
|---|-------|------|
"""
            for i, r in enumerate(all_records[:15]):
                score = r.get('score', 0)
                content_text = r.get('content', '')[:40]
                mem_type = r['_type'][0].upper()  # F/B/S
                snapshot += f"| {i+1} | {score:.2f} | [{mem_type}] {content_text} |\n"
            
            snapshot += f"""
## ğŸ·ï¸ å®ä½“ç´¢å¼•
"""
            for entity in sorted(all_entities)[:10]:
                related = len([r for r in all_records if entity in r.get('entities', [])])
                snapshot += f"- **{entity}**: {related} æ¡ç›¸å…³è®°å¿†\n"
            
            snapshot += f"""
## ğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ
- **Facts**: {facts_count} æ¡ ({facts_count*100//max(len(all_records),1)}%)
- **Beliefs**: {beliefs_count} æ¡ ({beliefs_count*100//max(len(all_records),1)}%)
- **Summaries**: {summaries_count} æ¡ ({summaries_count*100//max(len(all_records),1)}%)
- **å…³é”®ä¿¡æ¯**: {len(critical)} æ¡
- **é‡è¦ä¿¡æ¯**: {len(high)} æ¡

---
*Memory System v1.0 | ä½¿ç”¨ memory_search æ£€ç´¢è¯¦ç»†ä¿¡æ¯*
"""
            
            with open(memory_dir / 'layer1/snapshot.md', 'w', encoding='utf-8') as f:
                f.write(snapshot)
            
            # ä¿å­˜æ’å
            rankings = [{'id': r['id'], 'score': r.get('score', 0)} for r in all_records[:50]]
            with open(memory_dir / 'state/rankings.json', 'w', encoding='utf-8') as f:
                json.dump({'updated': now_iso(), 'rankings': rankings}, f, indent=2, ensure_ascii=False)
            
            print("   âœ… å®Œæˆ")
        
        # æ›´æ–°æˆåŠŸçŠ¶æ€
        state['last_success'] = now_iso()
        state['current_phase'] = None
        state['retry_count'] = 0
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        
        print("\n" + "=" * 40)
        print("âœ… Consolidation å®Œæˆ!")
        
    except Exception as e:
        state['retry_count'] = state.get('retry_count', 0) + 1
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        print(f"\nâŒ Consolidation å¤±è´¥: {e}")
        raise

# ============================================================
# ç»´æŠ¤å‘½ä»¤
# ============================================================

def cmd_rebuild_index(args):
    """é‡å»ºç´¢å¼•"""
    memory_dir = get_memory_dir()
    print("ğŸ”„ é‡å»ºç´¢å¼•...")
    
    # è°ƒç”¨ Phase 6 é€»è¾‘
    args.phase = 6
    args.force = True
    cmd_consolidate(args)

def cmd_validate(args):
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    memory_dir = get_memory_dir()
    print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    errors = []
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    required_dirs = [
        'layer1', 'layer2/active', 'layer2/archive',
        'layer2/entities', 'layer2/index', 'state'
    ]
    for d in required_dirs:
        if not (memory_dir / d).exists():
            errors.append(f"ç¼ºå°‘ç›®å½•: {d}")
    
    # æ£€æŸ¥ JSONL æ–‡ä»¶æ ¼å¼
    for mem_type in ['facts', 'beliefs', 'summaries']:
        for pool in ['active', 'archive']:
            path = memory_dir / f'layer2/{pool}/{mem_type}.jsonl'
            if path.exists():
                try:
                    records = load_jsonl(path)
                    for i, r in enumerate(records):
                        if 'id' not in r:
                            errors.append(f"{path}:{i+1} ç¼ºå°‘ id å­—æ®µ")
                        if 'content' not in r:
                            errors.append(f"{path}:{i+1} ç¼ºå°‘ content å­—æ®µ")
                except Exception as e:
                    errors.append(f"{path} è§£æå¤±è´¥: {e}")
    
    if errors:
        print(f"âŒ å‘ç° {len(errors)} ä¸ªé—®é¢˜:")
        for e in errors[:10]:
            print(f"   - {e}")
        if len(errors) > 10:
            print(f"   ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé—®é¢˜")
    else:
        print("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")

# ============================================================
# ä¸»å…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description='Memory System v1.0 - ä¸‰å±‚è®°å¿†æ¶æ„ CLI',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # init
    parser_init = subparsers.add_parser('init', help='åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ')
    parser_init.set_defaults(func=cmd_init)
    
    # status
    parser_status = subparsers.add_parser('status', help='æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€')
    parser_status.set_defaults(func=cmd_status)
    
    # stats
    parser_stats = subparsers.add_parser('stats', help='æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡')
    parser_stats.set_defaults(func=cmd_stats)
    
    # capture
    parser_capture = subparsers.add_parser('capture', help='æ‰‹åŠ¨æ·»åŠ è®°å¿†')
    parser_capture.add_argument('content', help='è®°å¿†å†…å®¹')
    parser_capture.add_argument('--type', choices=['fact', 'belief', 'summary'], default='fact', help='è®°å¿†ç±»å‹')
    parser_capture.add_argument('--importance', type=float, default=0.5, help='é‡è¦æ€§ (0-1)')
    parser_capture.add_argument('--confidence', type=float, default=0.6, help='ç½®ä¿¡åº¦ (belief ä¸“ç”¨)')
    parser_capture.add_argument('--entities', default='', help='ç›¸å…³å®ä½“ï¼Œé€—å·åˆ†éš”')
    parser_capture.set_defaults(func=cmd_capture)
    
    # archive
    parser_archive = subparsers.add_parser('archive', help='æ‰‹åŠ¨å½’æ¡£è®°å¿†')
    parser_archive.add_argument('id', help='è®°å¿† ID')
    parser_archive.set_defaults(func=cmd_archive)
    
    # consolidate
    parser_consolidate = subparsers.add_parser('consolidate', help='æ‰§è¡Œ Consolidation')
    parser_consolidate.add_argument('--force', action='store_true', help='å¼ºåˆ¶æ‰§è¡Œ')
    parser_consolidate.add_argument('--phase', type=int, choices=[1,2,3,4,5,6,7], help='åªæ‰§è¡ŒæŒ‡å®šé˜¶æ®µ')
    parser_consolidate.add_argument('--input', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆPhase 1 æ•°æ®æºï¼‰')
    parser_consolidate.set_defaults(func=cmd_consolidate)
    
    # rebuild-index
    parser_rebuild = subparsers.add_parser('rebuild-index', help='é‡å»ºç´¢å¼•')
    parser_rebuild.set_defaults(func=cmd_rebuild_index)
    
    # validate
    parser_validate = subparsers.add_parser('validate', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
    parser_validate.set_defaults(func=cmd_validate)
    
    # search
    parser_search = subparsers.add_parser('search', help='æ™ºèƒ½æ£€ç´¢è®°å¿†')
    parser_search.add_argument('query', help='æ£€ç´¢æŸ¥è¯¢')
    parser_search.add_argument('--json', action='store_true', help='è¾“å‡º JSON æ ¼å¼')
    parser_search.set_defaults(func=cmd_search)
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    args.func(args)

if __name__ == '__main__':
    main()
