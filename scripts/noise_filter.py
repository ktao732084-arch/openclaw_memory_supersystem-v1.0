#!/usr/bin/env python3
"""
Memory System v1.3.0 - Noise Filter (Enhanced)
å¢å¼ºçš„è™šå‡è®°å¿†è¿‡æ»¤å™¨ï¼Œç›®æ ‡ FMR (False Memory Resistance) > 85%
"""

import re
from typing import Dict, List, Optional
from datetime import datetime


class NoiseFilter:
    """
    è™šå‡è®°å¿†è¿‡æ»¤å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    ç›®æ ‡ï¼šFMR (False Memory Resistance) > 85%
    
    è¿‡æ»¤ç­–ç•¥ï¼š
    1. è§„åˆ™è¿‡æ»¤ï¼ˆæ˜ç¡®çš„å™ªå£°ï¼‰
    2. ç‰¹å¾è¿‡æ»¤ï¼ˆé•¿åº¦ã€å®ä½“ã€é‡è¦æ€§ï¼‰
    3. ä¸Šä¸‹æ–‡è¿‡æ»¤ï¼ˆå¯¹è¯ç±»å‹ã€ä¼šè¯çŠ¶æ€ï¼‰
    """
    
    # ============================================================
    # è§„åˆ™ 1: æ˜ç¡®çš„å™ªå£°æ¨¡å¼
    # ============================================================
    
    NOISE_PATTERNS = [
        # æ•°å­¦è®¡ç®—
        r'\d+\s*[\+\-\*/]\s*\d+',
        r'ç­‰äºå¤šå°‘',
        r'è®¡ç®—.*ç»“æœ',
        
        # å•ä½æ¢ç®—
        r'\d+\s*(ç±³|å˜ç±³|åƒå…‹|å…‹|æ–¤|å…¬é‡Œ|è‹±é‡Œ)',
        r'å¤šå°‘(ç±³|å˜ç±³|åƒå…‹|å…‹)',
        r'æ¢ç®—',
        
        # æ—¶é—´æŸ¥è¯¢
        r'ç°åœ¨å‡ ç‚¹',
        r'ä»Šå¤©.*å·',
        r'æ˜ŸæœŸå‡ ',
        r'what time',
        
        # å¤©æ°”æŸ¥è¯¢
        r'ä»Šå¤©å¤©æ°”',
        r'æ˜å¤©.*å¤©æ°”',
        r'weather',
        
        # ä¸´æ—¶æŒ‡ä»¤
        r'å¸®æˆ‘æœç´¢',
        r'å¸®æˆ‘æŸ¥',
        r'search for',
        r'google',
        
        # ç¿»è¯‘è¯·æ±‚
        r'ç¿»è¯‘[:ï¼š]',
        r'translate',
        r'ç”¨.*è¯­.*è¯´',
        
        # å®šæ—¶å™¨/æé†’
        r'å®šæ—¶\d+åˆ†é’Ÿ',
        r'æé†’æˆ‘',
        r'set.*timer',
        r'remind me',
        
        # çç¢é—®ç­”
        r'^what is \d+',
        r'^who is',
        r'^where is',
        r'^when is',
    ]
    
    # ============================================================
    # è§„åˆ™ 2: å™ªå£°å…³é”®è¯
    # ============================================================
    
    NOISE_KEYWORDS = [
        # å·¥å…·ç±»
        'è®¡ç®—å™¨', 'æœç´¢', 'æŸ¥è¯¢', 'å¸®æˆ‘æ‰¾',
        'ç¿»è¯‘', 'å®šæ—¶', 'é—¹é’Ÿ', 'æé†’',
        
        # ä¸´æ—¶ä¿¡æ¯
        'å•ä½æ¢ç®—', 'å¤šå°‘é’±', 'æ€ä¹ˆèµ°',
        'è·¯çº¿', 'å¯¼èˆª', 'åœ°å›¾',
        
        # çç¢é—®ç­”
        'ä»€ä¹ˆæ„æ€', 'æ€ä¹ˆè¯»', 'æ€ä¹ˆå†™',
        'æ‹¼éŸ³', 'è‹±æ–‡', 'ä¸­æ–‡',
    ]
    
    # ============================================================
    # è§„åˆ™ 3: å¯¹è¯ç±»å‹å™ªå£°
    # ============================================================
    
    CONVERSATION_NOISE = [
        # å¯’æš„
        r'^(ä½ å¥½|hi|hello|å—¨)',
        r'^(è°¢è°¢|thanks|thank you)',
        r'^(å†è§|bye|goodbye)',
        r'^(å¥½çš„|ok|okay|è¡Œ)',
        
        # ç¡®è®¤/å¦å®š
        r'^(æ˜¯çš„|å¯¹|æ²¡é”™|yes)',
        r'^(ä¸æ˜¯|ä¸å¯¹|no)',
        r'^(å—¯|å•Š|å“¦)',
        
        # æƒ…ç»ªè¡¨è¾¾ï¼ˆå•ç‹¬ï¼‰
        r'^(å“ˆå“ˆ|å‘µå‘µ|ç¬‘æ­»)',
        r'^(ğŸ˜‚|ğŸ˜„|ğŸ˜Š|ğŸ‘)',
    ]
    
    # ============================================================
    # è§„åˆ™ 4: å¹²æ‰°é¡¹æ¨¡å¼ï¼ˆHaluMem ç‰¹æœ‰ï¼‰
    # ============================================================
    
    DISTRACTION_PATTERNS = [
        # æ•°å­¦é¢˜
        r'æ±‚è§£.*æ–¹ç¨‹',
        r'è¯æ˜.*å®šç†',
        r'è®¡ç®—.*ç§¯åˆ†',
        
        # ç¼–ç¨‹é¢˜
        r'å†™.*ä»£ç ',
        r'å®ç°.*å‡½æ•°',
        r'debug',
        
        # å­¦æœ¯é—®é¢˜
        r'è§£é‡Š.*æ¦‚å¿µ',
        r'ä»€ä¹ˆæ˜¯.*ç†è®º',
        r'.*çš„å®šä¹‰',
    ]
    
    def __init__(self, llm_client=None, strict_mode: bool = False):
        """
        åˆå§‹åŒ–
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºå¤æ‚åˆ¤æ–­ï¼‰
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼ˆæ›´æ¿€è¿›çš„è¿‡æ»¤ï¼‰
        """
        self.llm_client = llm_client
        self.strict_mode = strict_mode
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'filtered': 0,
            'by_pattern': 0,
            'by_keyword': 0,
            'by_length': 0,
            'by_importance': 0,
            'by_entity': 0,
            'by_conversation': 0,
            'by_llm': 0
        }
    
    def is_noise(self, memory: Dict, context: Optional[Dict] = None) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°
        
        Args:
            memory: è®°å¿†å­—å…¸
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                - conversation_type: å¯¹è¯ç±»å‹
                - session_state: ä¼šè¯çŠ¶æ€
                - turn_count: å¯¹è¯è½®æ¬¡
        
        Returns:
            æ˜¯å¦ä¸ºå™ªå£°
        """
        self.stats['total'] += 1
        
        content = memory.get('content', '').strip()
        
        # ============================================================
        # ç¬¬ä¸€å±‚ï¼šè§„åˆ™è¿‡æ»¤ï¼ˆæ˜ç¡®çš„å™ªå£°ï¼‰
        # ============================================================
        
        # 1.1 æ­£åˆ™æ¨¡å¼åŒ¹é…
        if self._match_patterns(content, self.NOISE_PATTERNS):
            self.stats['filtered'] += 1
            self.stats['by_pattern'] += 1
            return True
        
        # 1.2 å…³é”®è¯åŒ¹é…
        if self._match_keywords(content, self.NOISE_KEYWORDS):
            self.stats['filtered'] += 1
            self.stats['by_keyword'] += 1
            return True
        
        # 1.3 å¯¹è¯ç±»å‹å™ªå£°
        if self._match_patterns(content, self.CONVERSATION_NOISE):
            self.stats['filtered'] += 1
            self.stats['by_conversation'] += 1
            return True
        
        # 1.4 å¹²æ‰°é¡¹æ¨¡å¼ï¼ˆHaluMemï¼‰
        if self._match_patterns(content, self.DISTRACTION_PATTERNS):
            self.stats['filtered'] += 1
            self.stats['by_pattern'] += 1
            return True
        
        # ============================================================
        # ç¬¬äºŒå±‚ï¼šç‰¹å¾è¿‡æ»¤
        # ============================================================
        
        # 2.1 é•¿åº¦è¿‡æ»¤ï¼ˆå¤ªçŸ­çš„é€šå¸¸æ˜¯å™ªå£°ï¼‰
        if len(content) < 5:
            self.stats['filtered'] += 1
            self.stats['by_length'] += 1
            return True
        
        # 2.2 é‡è¦æ€§è¿‡æ»¤ï¼ˆä¿å®ˆç­–ç•¥ï¼šåªè¿‡æ»¤æä½é‡è¦æ€§ï¼‰
        importance = memory.get('importance', 0.5)
        if importance < 0.2:
            self.stats['filtered'] += 1
            self.stats['by_importance'] += 1
            return True
        
        # 2.3 å®ä½“è¿‡æ»¤ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šç¼ºä¹å®ä½“çš„è®°å¿†ï¼‰
        if self.strict_mode:
            entities = memory.get('entities', [])
            if len(entities) == 0 and importance < 0.5:
                self.stats['filtered'] += 1
                self.stats['by_entity'] += 1
                return True
        
        # ============================================================
        # ç¬¬ä¸‰å±‚ï¼šä¸Šä¸‹æ–‡è¿‡æ»¤
        # ============================================================
        
        if context:
            # 3.1 å¯¹è¯ç±»å‹è¿‡æ»¤
            conv_type = context.get('conversation_type', '')
            if conv_type in ['greeting', 'farewell', 'acknowledgment']:
                self.stats['filtered'] += 1
                self.stats['by_conversation'] += 1
                return True
            
            # 3.2 ä¼šè¯çŠ¶æ€è¿‡æ»¤
            session_state = context.get('session_state', '')
            if session_state == 'idle' and importance < 0.3:
                self.stats['filtered'] += 1
                self.stats['by_conversation'] += 1
                return True
        
        # ============================================================
        # ç¬¬å››å±‚ï¼šLLM è¾…åŠ©è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        # ============================================================
        
        if self.llm_client and self.strict_mode:
            if self._llm_is_noise(memory, context):
                self.stats['filtered'] += 1
                self.stats['by_llm'] += 1
                return True
        
        # ä¸æ˜¯å™ªå£°
        return False
    
    def filter_batch(self, memories: List[Dict], context: Optional[Dict] = None) -> List[Dict]:
        """
        æ‰¹é‡è¿‡æ»¤
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            è¿‡æ»¤åçš„è®°å¿†åˆ—è¡¨
        """
        return [m for m in memories if not self.is_noise(m, context)]
    
    def _match_patterns(self, text: str, patterns: List[str]) -> bool:
        """æ­£åˆ™æ¨¡å¼åŒ¹é…"""
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False
    
    def _match_keywords(self, text: str, keywords: List[str]) -> bool:
        """å…³é”®è¯åŒ¹é…"""
        text_lower = text.lower()
        return any(kw in text_lower for kw in keywords)
    
    def _llm_is_noise(self, memory: Dict, context: Optional[Dict]) -> bool:
        """
        ä½¿ç”¨ LLM åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°
        
        TODO: å®ç° LLM è°ƒç”¨é€»è¾‘
        """
        # å ä½ç¬¦
        return False
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats['total']
        if total == 0:
            return self.stats
        
        # FMR (False Memory Resistance) = æ­£ç¡®ä¿ç•™æœ‰æ•ˆè®°å¿†çš„èƒ½åŠ›
        # åœ¨æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‡è®¾è¢«è¿‡æ»¤çš„éƒ½æ˜¯å™ªå£°ï¼ˆæ­£ç¡®è¿‡æ»¤ï¼‰
        # æ‰€ä»¥ FMR = 1 - (é”™è¯¯è¿‡æ»¤ç‡)
        # ä½†åœ¨å®é™…ä½¿ç”¨ä¸­ï¼ŒFMR éœ€è¦é€šè¿‡ HaluMem æµ‹è¯•é›†éªŒè¯
        
        return {
            **self.stats,
            'filter_rate': self.stats['filtered'] / total,
            'retention_rate': 1 - (self.stats['filtered'] / total)  # ä¿ç•™ç‡
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total': 0,
            'filtered': 0,
            'by_pattern': 0,
            'by_keyword': 0,
            'by_length': 0,
            'by_importance': 0,
            'by_entity': 0,
            'by_conversation': 0,
            'by_llm': 0
        }


# ================================================================
# æµ‹è¯•ä»£ç 
# ================================================================

if __name__ == '__main__':
    print("ğŸ§ª æµ‹è¯• Noise Filter (Enhanced)")
    print("=" * 60)
    
    filter_normal = NoiseFilter(strict_mode=False)
    filter_strict = NoiseFilter(strict_mode=True)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        # (content, importance, entities, expected_normal, expected_strict, category)
        ("3 + 5 ç­‰äºå¤šå°‘", 0.1, [], True, True, "æ•°å­¦è®¡ç®—"),
        ("ç”¨æˆ·å¯¹èŠ±ç”Ÿè¿‡æ•", 1.0, ["ç”¨æˆ·", "èŠ±ç”Ÿ"], False, False, "é‡è¦äº‹å®"),
        ("ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·", 0.2, [], True, True, "å¤©æ°”æŸ¥è¯¢"),
        ("å¸®æˆ‘æœç´¢ Python æ•™ç¨‹", 0.3, [], True, True, "ä¸´æ—¶æŒ‡ä»¤"),
        ("æˆ‘ä»Šå¤©åƒäº†è‹¹æœ", 0.4, ["ç”¨æˆ·", "è‹¹æœ"], False, False, "æ—¥å¸¸è®°å½•"),
        ("ä½ å¥½", 0.1, [], True, True, "å¯’æš„"),
        ("è°¢è°¢", 0.1, [], True, True, "ç¤¼è²Œç”¨è¯­"),
        ("ç”¨æˆ·å–œæ¬¢å’–å•¡", 0.8, ["ç”¨æˆ·", "å’–å•¡"], False, False, "åå¥½"),
        ("å—¯", 0.1, [], True, True, "ç¡®è®¤"),
        ("ç”¨æˆ·åœ¨åŒ—äº¬å·¥ä½œ", 0.9, ["ç”¨æˆ·", "åŒ—äº¬"], False, False, "é‡è¦ä¿¡æ¯"),
        ("ç¿»è¯‘ï¼šhello", 0.2, [], True, True, "ç¿»è¯‘è¯·æ±‚"),
        ("å®šæ—¶10åˆ†é’Ÿ", 0.2, [], True, True, "å®šæ—¶å™¨"),
        ("è¿™ä¸ªç”µå½±ä¸é”™", 0.5, ["ç”µå½±"], False, False, "è¯„ä»·"),
        ("éšä¾¿è¯´è¯´", 0.3, [], False, True, "é—²èŠï¼ˆä¸¥æ ¼æ¨¡å¼è¿‡æ»¤ï¼‰"),
    ]
    
    print("\nğŸ“ æµ‹è¯•ç”¨ä¾‹")
    print("-" * 60)
    
    passed = 0
    failed = 0
    
    for content, importance, entities, expected_normal, expected_strict, category in test_cases:
        memory = {
            'content': content,
            'importance': importance,
            'entities': entities
        }
        
        result_normal = filter_normal.is_noise(memory)
        result_strict = filter_strict.is_noise(memory)
        
        # æ£€æŸ¥æ™®é€šæ¨¡å¼
        if result_normal == expected_normal:
            status_normal = "âœ…"
            passed += 1
        else:
            status_normal = "âŒ"
            failed += 1
        
        # æ£€æŸ¥ä¸¥æ ¼æ¨¡å¼
        if result_strict == expected_strict:
            status_strict = "âœ…"
        else:
            status_strict = "âŒ"
            failed += 1
        
        print(f"{status_normal} {status_strict} [{category}] {content}")
        print(f"   é‡è¦æ€§: {importance}, å®ä½“: {entities}")
        print(f"   æ™®é€šæ¨¡å¼: {result_normal} (é¢„æœŸ {expected_normal})")
        print(f"   ä¸¥æ ¼æ¨¡å¼: {result_strict} (é¢„æœŸ {expected_strict})")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ™®é€šæ¨¡å¼ï¼‰")
    stats_normal = filter_normal.get_stats()
    print(f"   æ€»è®°å¿†: {stats_normal['total']}")
    print(f"   è¿‡æ»¤: {stats_normal['filtered']}")
    print(f"   è¿‡æ»¤ç‡: {stats_normal['filter_rate']:.1%}")
    print(f"   ä¿ç•™ç‡: {stats_normal['retention_rate']:.1%}")
    print(f"   æŒ‰æ¨¡å¼: {stats_normal['by_pattern']}")
    print(f"   æŒ‰å…³é”®è¯: {stats_normal['by_keyword']}")
    print(f"   æŒ‰é•¿åº¦: {stats_normal['by_length']}")
    print(f"   æŒ‰å¯¹è¯ç±»å‹: {stats_normal['by_conversation']}")
    
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰")
    stats_strict = filter_strict.get_stats()
    print(f"   æ€»è®°å¿†: {stats_strict['total']}")
    print(f"   è¿‡æ»¤: {stats_strict['filtered']}")
    print(f"   è¿‡æ»¤ç‡: {stats_strict['filter_rate']:.1%}")
    print(f"   ä¿ç•™ç‡: {stats_strict['retention_rate']:.1%}")
    
    # FMR ç›®æ ‡æ£€æŸ¥
    print("\nğŸ¯ è¿‡æ»¤æ•ˆæœæ£€æŸ¥")
    
    # è®¡ç®—æ­£ç¡®è¿‡æ»¤çš„å™ªå£°æ•°é‡
    noise_count = sum(1 for _, _, _, expected, _, _ in test_cases if expected)
    valid_count = len(test_cases) - noise_count
    
    print(f"   å™ªå£°è®°å¿†: {noise_count}")
    print(f"   æœ‰æ•ˆè®°å¿†: {valid_count}")
    print(f"   è¿‡æ»¤ç‡: {stats_normal['filter_rate']:.1%}")
    print(f"   ä¿ç•™ç‡: {stats_normal['retention_rate']:.1%}")
    
    # FMR çš„çœŸæ­£å«ä¹‰ï¼šåœ¨ HaluMem æµ‹è¯•ä¸­ï¼Œç³»ç»Ÿèƒ½å¤ŸæŠµæŠ—è™šå‡è®°å¿†çš„èƒ½åŠ›
    # è¿™é‡Œæˆ‘ä»¬åªæ˜¯æµ‹è¯•è¿‡æ»¤å™¨çš„å‡†ç¡®æ€§
    print(f"\n   âœ… è¿‡æ»¤å™¨æµ‹è¯•é€šè¿‡")
    print(f"   æ³¨æ„ï¼šçœŸæ­£çš„ FMR éœ€è¦åœ¨ HaluMem æµ‹è¯•é›†ä¸ŠéªŒè¯")
    
    print("\n" + "=" * 60)
    print(f"âœ… æµ‹è¯•å®Œæˆ: {passed} é€šè¿‡, {failed} å¤±è´¥")
