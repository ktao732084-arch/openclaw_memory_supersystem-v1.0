#!/usr/bin/env python3
"""
Memory System v1.8.0 - æ•°æ®è¿ç§»è„šæœ¬
ä»å•æ–‡ä»¶ SQLite/JSONL è¿ç§»åˆ°åˆ†ç‰‡å­˜å‚¨

ç”¨æ³•:
    python migrate_to_sharded.py <memory_dir> [--shard-size 10000] [--backup]
"""

import argparse
import json
import shutil
import sqlite3
from pathlib import Path
from typing import Optional


def migrate_jsonl_to_sharded(
    memory_dir: Path,
    shard_dir: Path,
    shard_size: int = 10000,
    backup: bool = True,
    progress_callback: Optional[callable] = None,
) -> tuple[int, int]:
    """
    ä» JSONL è¿ç§»åˆ°åˆ†ç‰‡å­˜å‚¨

    è¿”å›: (æˆåŠŸæ•°, å¤±è´¥æ•°)
    """
    from sharded_index import ShardedIndexManager

    shard_dir.mkdir(parents=True, exist_ok=True)
    manager = ShardedIndexManager(shard_dir, shard_size)

    success_count = 0
    fail_count = 0

    for mem_type in ["facts", "beliefs", "summaries"]:
        jsonl_path = memory_dir / "layer2" / "active" / f"{mem_type}.jsonl"

        if not jsonl_path.exists():
            continue

        if backup:
            backup_path = jsonl_path.with_suffix(".jsonl.migration_backup")
            if not backup_path.exists():
                shutil.copy2(jsonl_path, backup_path)
                print(f"âœ… å¤‡ä»½: {jsonl_path} -> {backup_path}")

        print(f"ğŸ“ è¿ç§» {mem_type}...")

        with open(jsonl_path, encoding="utf-8") as f:
            batch = []
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue

                try:
                    record = json.loads(line)
                    record["type"] = mem_type.rstrip("s")

                    batch.append(record)

                    if len(batch) >= 1000:
                        for m in batch:
                            try:
                                manager.insert(m)
                                success_count += 1
                            except Exception as e:
                                print(f"   âš ï¸ æ’å…¥å¤±è´¥: {m.get('id', 'unknown')}: {e}")
                                fail_count += 1

                        if progress_callback:
                            progress_callback(success_count, fail_count)

                        batch = []

                except json.JSONDecodeError as e:
                    print(f"   âš ï¸ JSON è§£æå¤±è´¥ (è¡Œ {line_num}): {e}")
                    fail_count += 1

            for m in batch:
                try:
                    manager.insert(m)
                    success_count += 1
                except Exception as e:
                    print(f"   âš ï¸ æ’å…¥å¤±è´¥: {m.get('id', 'unknown')}: {e}")
                    fail_count += 1

    stats = manager.get_stats()
    print("\nğŸ“Š è¿ç§»ç»Ÿè®¡:")
    print(f"   æ€»è®°å¿†æ•°: {stats['total_memories']}")
    print(f"   åˆ†ç‰‡æ•°: {stats['shard_count']}")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {fail_count}")

    manager.close()

    return success_count, fail_count


def migrate_sqlite_to_sharded(
    sqlite_path: Path,
    shard_dir: Path,
    shard_size: int = 10000,
    backup: bool = True,
    progress_callback: Optional[callable] = None,
) -> tuple[int, int]:
    """
    ä»å•æ–‡ä»¶ SQLite è¿ç§»åˆ°åˆ†ç‰‡å­˜å‚¨

    è¿”å›: (æˆåŠŸæ•°, å¤±è´¥æ•°)
    """
    from sharded_index import ShardedIndexManager

    if not sqlite_path.exists():
        print(f"âŒ SQLite æ–‡ä»¶ä¸å­˜åœ¨: {sqlite_path}")
        return 0, 0

    if backup:
        backup_path = sqlite_path.with_suffix(".db.migration_backup")
        if not backup_path.exists():
            shutil.copy2(sqlite_path, backup_path)
            print(f"âœ… å¤‡ä»½: {sqlite_path} -> {backup_path}")

    shard_dir.mkdir(parents=True, exist_ok=True)
    manager = ShardedIndexManager(shard_dir, shard_size)

    conn = sqlite3.connect(sqlite_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    success_count = 0
    fail_count = 0

    try:
        cursor.execute("SELECT * FROM memories WHERE state = 0")

        batch = []
        for row in cursor.fetchall():
            try:
                record = {
                    "id": row["id"],
                    "type": row["type"],
                    "content": row["content"],
                    "importance": row["importance"],
                    "confidence": row["confidence"],
                    "score": row["score"],
                    "entities": json.loads(row["entities"]) if row["entities"] else [],
                    "metadata": json.loads(row["metadata"]) if row["metadata"] else {},
                    "created_at": row["created_at"],
                    "updated_at": row["updated_at"],
                    "access_count": row["access_count"],
                }

                batch.append(record)

                if len(batch) >= 1000:
                    for m in batch:
                        try:
                            manager.insert(m)
                            success_count += 1
                        except Exception:
                            fail_count += 1

                    if progress_callback:
                        progress_callback(success_count, fail_count)

                    batch = []

            except Exception as e:
                print(f"   âš ï¸ å¤„ç†è®°å½•å¤±è´¥: {row['id']}: {e}")
                fail_count += 1

        for m in batch:
            try:
                manager.insert(m)
                success_count += 1
            except Exception:
                fail_count += 1

    finally:
        conn.close()

    stats = manager.get_stats()
    print("\nğŸ“Š è¿ç§»ç»Ÿè®¡:")
    print(f"   æ€»è®°å¿†æ•°: {stats['total_memories']}")
    print(f"   åˆ†ç‰‡æ•°: {stats['shard_count']}")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {fail_count}")

    manager.close()

    return success_count, fail_count


def verify_migration(source_path: Path, shard_dir: Path, sample_size: int = 100) -> dict:
    """
    éªŒè¯è¿ç§»ç»“æœ

    è¿”å›éªŒè¯æŠ¥å‘Š
    """
    from sharded_index import ShardedIndexManager

    manager = ShardedIndexManager(shard_dir)

    report = {
        "source_type": None,
        "source_count": 0,
        "target_count": 0,
        "verified_count": 0,
        "missing_count": 0,
        "corrupted_count": 0,
        "sample_verified": 0,
        "success": False,
    }

    jsonl_count = 0
    for mem_type in ["facts", "beliefs", "summaries"]:
        jsonl_path = source_path / "layer2" / "active" / f"{mem_type}.jsonl"
        if jsonl_path.exists():
            with open(jsonl_path, encoding="utf-8") as f:
                jsonl_count += sum(1 for line in f if line.strip())

    sqlite_path = source_path / "layer2" / "memories.db"
    sqlite_count = 0
    if sqlite_path.exists():
        conn = sqlite3.connect(sqlite_path)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM memories WHERE state = 0")
        sqlite_count = cursor.fetchone()[0]
        conn.close()

    if jsonl_count > 0:
        report["source_type"] = "jsonl"
        report["source_count"] = jsonl_count
    elif sqlite_count > 0:
        report["source_type"] = "sqlite"
        report["source_count"] = sqlite_count

    stats = manager.get_stats()
    report["target_count"] = stats["total_memories"]

    if sqlite_path.exists():
        conn = sqlite3.connect(sqlite_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT id FROM memories WHERE state = 0 LIMIT ?", (sample_size,))

        for row in cursor.fetchall():
            memory_id = row["id"]
            target_memory = manager.get_by_id(memory_id)

            if target_memory is None:
                report["missing_count"] += 1
            else:
                report["verified_count"] += 1

        conn.close()
        report["sample_verified"] = sample_size

    report["success"] = (
        report["missing_count"] == 0
        and report["corrupted_count"] == 0
        and report["target_count"] >= report["source_count"] * 0.99
    )

    manager.close()

    return report


def print_migration_report(report: dict):
    """æ‰“å°è¿ç§»æŠ¥å‘Š"""
    print("\n" + "=" * 50)
    print("ğŸ“‹ è¿ç§»éªŒè¯æŠ¥å‘Š")
    print("=" * 50)
    print(f"æºç±»å‹: {report['source_type']}")
    print(f"æºè®°å½•æ•°: {report['source_count']}")
    print(f"ç›®æ ‡è®°å½•æ•°: {report['target_count']}")
    print(f"éªŒè¯æ ·æœ¬: {report['sample_verified']}")
    print(f"éªŒè¯é€šè¿‡: {report['verified_count']}")
    print(f"ç¼ºå¤±è®°å½•: {report['missing_count']}")
    print(f"æŸåè®°å½•: {report['corrupted_count']}")
    print("=" * 50)

    if report["success"]:
        print("âœ… è¿ç§»éªŒè¯é€šè¿‡")
    else:
        print("âš ï¸ è¿ç§»éªŒè¯å‘ç°é—®é¢˜")


def main():
    parser = argparse.ArgumentParser(
        description="Memory System æ•°æ®è¿ç§»å·¥å…·", formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument("memory_dir", help="è®°å¿†ç³»ç»Ÿç›®å½•")

    parser.add_argument("--shard-size", type=int, default=10000, help="åˆ†ç‰‡å¤§å°ï¼ˆé»˜è®¤ 10000ï¼‰")

    parser.add_argument("--backup", action="store_true", default=True, help="å¤‡ä»½åŸå§‹æ•°æ®ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")

    parser.add_argument("--no-backup", action="store_true", help="ä¸å¤‡ä»½åŸå§‹æ•°æ®")

    parser.add_argument("--verify", action="store_true", help="è¿ç§»åéªŒè¯")

    parser.add_argument("--verify-only", action="store_true", help="ä»…éªŒè¯ï¼ˆä¸æ‰§è¡Œè¿ç§»ï¼‰")

    args = parser.parse_args()

    memory_dir = Path(args.memory_dir)
    shard_dir = memory_dir / "shards"

    backup = args.backup and not args.no_backup

    if args.verify_only:
        print("ğŸ” ä»…éªŒè¯æ¨¡å¼")
        report = verify_migration(memory_dir, shard_dir)
        print_migration_report(report)
        return

    print("ğŸš€ å¼€å§‹è¿ç§»...")
    print(f"   æºç›®å½•: {memory_dir}")
    print(f"   ç›®æ ‡ç›®å½•: {shard_dir}")
    print(f"   åˆ†ç‰‡å¤§å°: {args.shard_size}")
    print(f"   å¤‡ä»½: {'æ˜¯' if backup else 'å¦'}")
    print()

    sqlite_path = memory_dir / "layer2" / "memories.db"

    if sqlite_path.exists():
        print("ğŸ“¦ æ£€æµ‹åˆ° SQLite æ•°æ®åº“ï¼Œä» SQLite è¿ç§»...")
        success, fail = migrate_sqlite_to_sharded(sqlite_path, shard_dir, args.shard_size, backup)
    else:
        print("ğŸ“¦ ä» JSONL æ–‡ä»¶è¿ç§»...")
        success, fail = migrate_jsonl_to_sharded(memory_dir, shard_dir, args.shard_size, backup)

    print(f"\nâœ… è¿ç§»å®Œæˆ: æˆåŠŸ {success}, å¤±è´¥ {fail}")

    if args.verify:
        print("\nğŸ” éªŒè¯è¿ç§»ç»“æœ...")
        report = verify_migration(memory_dir, shard_dir)
        print_migration_report(report)


if __name__ == "__main__":
    main()
