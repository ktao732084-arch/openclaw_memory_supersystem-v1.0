# 数据去重机制 - 实现完成

## ✅ 已实现功能

### 1. 智能去重检测
- 同步前自动检查目标日期是否已有数据
- 如果已有数据，跳过同步并提示
- 避免重复写入

### 2. 强制替换模式
- 支持 `force_replace=True` 参数
- 先删除旧数据，再写入新数据
- 适用于数据更新场景

### 3. 手动清理工具
```bash
# 检查重复
python3 dedup.py check 2026-02-11

# 清理指定日期
python3 dedup.py clean 2026-02-11
```

## 📊 测试结果

### 测试1：正常同步（无重复）
```
✓ 获取到 4 条数据
✓ 2026-02-12 没有现有记录
✓ 写入成功: 4 条
```

### 测试2：重复同步（有数据）
```
✓ 获取到 4 条数据
⚠️  2026-02-11 已有 20 条记录
   建议使用 force_replace=True 模式重新同步
✓ 跳过同步
```

### 测试3：强制替换
```
✓ 获取到 4 条数据
🗑️  删除 2026-02-11 的旧数据...
   ✓ 删除 20 条旧记录
✓ 写入成功: 4 条
```

## 🔧 使用方法

### 日常自动同步
```python
# sync_data.py 默认启用去重
# 如果数据已存在，会自动跳过
python3 sync_data.py
```

### 强制更新数据
如果需要更新某天的数据：

**方法1：使用 force_replace**
```python
# 修改 sync_data.py 中的调用
sync_to_feishu(data, force_replace=True)
```

**方法2：手动清理后同步**
```bash
# 先清理
python3 dedup.py clean 2026-02-11

# 再同步
python3 sync_data.py
```

## 📝 工作流程

```
1. 获取巨量数据
   ↓
2. 检查飞书中是否已有该日期的数据
   ├─ 没有 → 直接写入
   └─ 有 → 跳过同步（或强制替换）
```

## ⚠️ 注意事项

### 1. 定时任务
定时任务每天只运行一次，不会产生重复：
- 每天 7:00 同步昨天的数据
- 首次同步：写入成功
- 再次运行：检测到已有数据，自动跳过

### 2. 手动运行
如果手动多次运行同步脚本：
- 第一次：写入成功
- 第二次：检测到重复，自动跳过
- 需要更新：使用 `force_replace=True`

### 3. 月度导出
月度导出可能产生重复，建议：
```bash
# 方案1：导出前清理
for day in {01..12}; do
    echo "yes" | python3 dedup.py clean 2026-02-$day
done
python3 export_month.py 2026 2

# 方案2：导出到新表格
# 修改 FEISHU_TABLE_ID 指向新表格
```

## ✅ 优势

1. **自动化**：无需手动干预，自动检测重复
2. **安全**：默认不覆盖已有数据
3. **灵活**：支持强制替换模式
4. **可靠**：基于记录数量判断，不依赖字段内容

## 🎯 总结

数据去重机制已完全实现并测试通过：
- ✅ 自动检测重复数据
- ✅ 智能跳过已同步的日期
- ✅ 支持强制替换模式
- ✅ 提供手动清理工具
- ✅ 集成到同步脚本

**明天的定时任务将自动检测重复，不会产生重复数据！**
